출처 : https://www.youtube.com/watch?v=b87I1plPeMg
# Agenda
- Claude는 어떻게 서빙될까?
- 큰 스케일에서의 비용 효율적인 인퍼런싱
# MLOps at Scale
- Anthropic은 "큰 규모"에서 MLOps를 수행한다.
    - 각각의 서버에서 신뢰성에 대한 챌린지가 된다.
    - GPU, TPU가 많고, 서버가 많으면 당연히 할일이 많아진다.
- Customer-triggered maintenance
    - Anthropic이 원하는 시간에 점검을 하도록 하는 것
    - GKE는 점검 요청을 받으면 Graceful하게 워크로드를 종료하며 자동으로 노드를 Drain한다.
        - 여러 노드에 걸쳐 대규모 워크로드가 있는 경우 파드 중단 비용같은 것을 고려할 수 있다.
    - 클로드는 화려한 CRD 기술 없이 StatefulSet으로 서빙되며, 클러스터 내부에 여러 개를 띄워 작동시킨다.
    - 다음과 같은 구조로 작동한다.
        image- GKE leader worker set
    - 2개의 다른 종류의 리소스가 필요한 서빙을 하는 경우 굉장히 유용하다.
        - 로드밸런서, 큐 서버가 있고 GPU와 TPU가 동시에 있는경우 로드밸런서와 큐 서버를 GPU에서 돌리면 굉장한 자원 낭비다.
        - 리더 워커 셋이 하나의 리더와 그를 따르는 StatefulSet을 갖도록 한다.
# Kubernetes for Claude
- 쿠버네티스는 여러가지 다른 종류의 어플리케이션에 대해 장점이 있다.
    - 클로드 같은 경우, 서비스 디스커버리가 굉장한 챌린지고, 쿠버네티스는 여기에 대해 명확한 정답을 준다.
- 다이어그램을 보자.
    image    - 재밌는 이야기지만, 프론트엔드에서 클라우드 런도 사용한다. 언젠간 다시 얘기하자.
    - 파드를 통해 추상화하면 사용량 최적화가 더 쉬워진다.
        - 모델에는 여러개의 레플리카가 여러개의 노드에 나뉘어 존재하고, 이를 컨테이너와 쿠버네티스만큼 잘 다루는 방법은 없다.
        - Pod Scheduling Readiness는 노드와 파드, 워크로드에 있어, 모든 스케줄러를 교체하지 않고도 사용자가 조작할 수 있도록 한다.
            - 스케줄러를 다시 돌리지 않고도 Topology같은 것을 지키도록 하고싶을때 편리하다.
            - 1.27버전부터 베타고 1.30부터 Stable인데, 끌어다 썼다.
    - GPU와 TPU가 혼재하는 환경에서, 워크로드가 어떤 가속기에서 작동할 것인지를 결정하는 것이 쿠버네티스에서 편리하게 구현될 수 있다.
        - 개별 노드를 관리할 필요도 없고, 드라이버같은 것들을 직접 묶어 줄 필요도 없이, GKE를 통해 자동으로 배포할 수 있다.
        - TPU 256개룰 엄청나게 빠른 속도의 메쉬로 구서하여 일부 모델을 위해 사용한다.
            -  TPU는 입력을 잘 쪼개서 처리하고, 초당 393테라 ops의 속도로 작동한다.
            image            - 다음과 같은 메쉬 구조로 구성되어, 빠르게 연결되며, Ring 연산을 수행할때 훌륭하다.
            - 클라우드 TPU가 아닌, GKE 내부에 구성해서 레버리지하였고, 잘 작동하였으며 인퍼런스 워크로드에서 높은 처리량을 보였다.
                - 빠른 응답속도, 효율적인 자원 사용, 쿠버네티스를 통한 쉬운 오케스트레이션
            - 고속 광학 Inter-chip interconnect(ICI)로 TPU VM을 연결하여 가속했다
            - JAX써서 리서치에서 사용한 모델 프로덕션에서 스케일업으로 작동시킬 수 있었다.
    - TPU를 GKE에서 돌리는 방법으로 노드 풀을 추상화할 수 있었다.
        image        - 노드 풀은 하나의 슬라이스에 해당하고, 이를 통해 워크 로드를 스케줄링할 떄 쉽게 처리할 수 있으며 노드 풀 기반으로 inter-pot affiinity를 구성할 수 있었다.
        - GPU보다 아키텍처가 간단하고 미스테리박스보단 퍼즐같은 형태였다.
            - Systolic Array가 GPU 쓰는것보다 디자인이 훨씬 우아했다.
    - GKE 클러스터 관리에는 테라폼 사용했고, TPU에 대해서도 잘 작동했다. 
    - TPU와 GPU로 GKE를 빠르게 서빙하도록 구현했다.
        - 에러 저항성, 회복 탄력성, 워크로드가 노드의 100%를 활용하도록 구성해서 고객에게 더 나은 환경을 제공했다.
            - 아직도 캐퍼 문제가 있다. 운영을 위해선 아직도 레이트 리밋이 필요하다.
# GKE Serving Stack
- GKE에서 대규모 AI/ML 추론을 비용 효율적으로 운영할 수 있는 여러 기능이 있다.
- GPU랑 TPU 계산을 잘 해야 한다.
    - 모델 별로 Throughtput Per pricing 계산해서 GPU 선택해야 한다.
      - L4가 A100보다 가성비 좋을수도 있다.
- GKE에서 다음과 같은 스택으로 모델이 서빙된다.
    image    - 큰 스케일에서 비용 효율적으로 서빙할 수 있는 방법이 많다.
    - 빠르게 성장하는 스타트업들을 위해 container pre-loading같은거 제공한다.
        - 시간마다 고객 요청 수 다르고, 예측이 어려운데 요청 늘어나면 VM띄워야 한다.
            - VM 띄우는 것부터 컨테이너 스케줄하고 띄우는것까지 다 시간인데, 스파이크 뜰때 이게 바로바로 안뜨면 의미가 없다.
            - 그렇다고 요청보다 더 많이 띄워두면 이것도 다 돈이다.
        - 컨테이너 이미지를 세컨더리 부트 디스크에 미리 로드해두고, 노드가 뜨면 바로 쿠버네티스에 스케줄될수 있도록 하는것
            - 네트워크 붙어서 엄청나게 큰 인퍼런싱 이미지 가져오는것보다 훨씬 빠르다.
        - 결국 이미지 가져오는 네트워크 패치 시간을 줄이는거다.
        - Vertex AI의 16기가짜리 이미지도 이거 사용해서 속도 29x 향상을 거두었다.
    - Cloud Storage FUSE 쓰면 호환성이나 파이썬 네이티브 파일 시멘틱같은게 좋아진다.
        - GCS보다 속도도 빨라용
        - 휴대성, GCS 파일 시멘틱 배울 필요 없어짐. 클라우드 스토리지로 빠른 데이터 획득까지
- GPU 타임 쉐어링으로 비용 낮출 수 있다.
    - 노트북이 있을 때, 요청수가 적은 인퍼런스 워크로드가 있을 수 있고, 서빙할 때 요청 수가 줄어들 수도 있다.
    - 쿠버네티스는 GPU마다 하나의 컨테이너를 갖는데, GPU 타임을 공유할 수 있다.
        - 복수 개의 컨테이너를 하나의 GPU에서 타임 쉐어로 띄워서 절약할 수 있다.
    - 사용량을 1.6배 높여 66%로 비용을 감소 시켰다.
- Nvidia Multi-Process Service도 출시했다.
    - 타임쉐어링과 비슷한 기능인데, 단점이 없다.
    - 여러 개의 프로세스에서 하나의 GPU를 나누어 쓰도록
    - GKE 오토파일럿에 GPU랑 TPU 지원도 추가해서, GPU TPU도 서버리스처럼 쓸 수 있다.
Deploy Gemma on GKE
- GTC에서 NIM Inference microservice를 출시했다
    - 텐서RA와 선택된 모델로 구성된 최적화된 마이크로서비스 프레임워크
    - 엔비디아와 협업하여 이런 스택을 구성했다.
- 오늘 구글이 오픈소스 LM 인퍼런스 스택 Jetstream을 출시했다..
    - JAX는 최적화만 담당하고, 서빙 스택과 JAX 모델을 통합하는 것이 Jetstream이다.
    - 다음과 같은 아키텍처로 구성할 수 있게 되었다
    image    - 스탠다드나 오토파일럿으로 구성된 GKE 클러스터에 2개의 노드풀이 있고, 각각의 노드풀엔 싱글 호스트 TPU 슬라이스를 둔다.
        - 각 노드 풀은 하나의 모델이 배포된다.
    - 쿠버네티스 파드는 2개의 구성 요소 컨테이너로 구성된다.
        - HTTP 요청을 처리하고 Max Engine에 필요한 포맷된 형태로 전송하는 HTTP 서버
        - gRPC로 데이터를 전송하고, Max Engine이 gRPC로 온 데이터를 받아 제마를 인퍼런싱한다.
    - Gemma 7B에 TPU 썼을 때, Jetstream 사용 안한것보다 Throughtput per cost 3x되었다.
- 
