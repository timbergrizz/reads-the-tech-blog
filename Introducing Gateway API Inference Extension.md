---
type: Study
title: Introducing Gateway API Inference Extension
tags: []
출처: '[Introducing Gateway API Inference Extension](https://kubernetes.io/blog/2025/06/05/introducing-gateway-api-inference-extension/)'
---

- 기존의 짧은 수명의 stateless 웹 요청과 다르게, LLM 인퍼런싱은 긴 수명의, 많은 자원을 요구하는 stateful한 세션을 필요로 한다.

    - 이러한 특성으로 인해 traffic-routing에 있어 챌린지가 생긴다.

- 전통적인 LB는 이러한 워크로드에 대한 특별한 능력 없이 HTTP 경로와 라운드 로빈에 집중한다.

    - 조직들은 애드혹 솔루션들을 사용했으나, 표준화된 접근법이 없었다.

# Gateway API Inference Extension

- Gateway API Inference Extension은 Gateway API의 확장으로, 인퍼런스 워크로드의 요구사항을 다루기 위해 설계되었다.

    - 기존 게이트웨이와 HTTP라우트와 비슷한 모델로 인퍼런스에 맞춘 라우팅 능력을 추가했다.

- 프로젝트의 목표는 전체 생태계에 인퍼런스 워크로드 라우팅을 향상하고 표준화 하는 것

    - 모델 인지 라우팅, 요청당 중요도, 안전한 모델 롤아웃, 실시간 모델 메트릭을 통한 로드밸런싱 최적화를 가능하게 하는 것이 목표

# How it works

[image](https://app.capacities.io/d1cf29da-a5be-492d-839a-e228b21e0129/de9b1c40-67a2-4314-918e-80f73d786c07)

- 디자인은 구분된 역할로 정의된 2개의 새로운 커스텀 리소스를 포함한다.

- InferencePool

    - 공유 컴퓨팅 환경에서(GPU 노드 등) 작동하고 있는 파드의 풀을 정의한다.

    - 어드민은 파드의 배포 방식, 리소스 요청, 수명, 오토스케일링 전략 등을 설정할 수 있다.

    - 지속적인 자원 사용을 보장하고 플랫폼 전반의 정책을 적용하도록 한다.

    - 서비스와 유사하지만, AI/ML 서빙의 요구와 모델 서빙 프로토콜의 특화되었다.

- InferenceModel

    - AI/ML의 관리자가 관리하는 유저가 마주하는 모델의 엔드포인트

    - 인퍼런스풀 내부의 실질적인 모델과 퍼블릭 네임을 매핑한다.

    - 워크로드 오너가 서빙하려는 모델, 버전, 사용자 정의 동작 등을 명시할 수 있다.

- InferenceModel은 AI/ML유저가 무엇이 서빙되는지를 관리하고, InferencePool은 플랫폼 관리자가 어디서 어떻게 서빙할것인지를 정의한다.

# Request Flow

[image](https://app.capacities.io/d1cf29da-a5be-492d-839a-e228b21e0129/ddec641d-0123-4dbd-a813-ad9befdfa494)

- 리퀘스트의 흐름은 Gateway API 모델에 기반하여 설계되었다.

    - 중간에 하나 이상의 인퍼런싱을 고려한 단계가 추가된다.

1. Gateway Routing

    - 클라이언트가 요청을 보내면, 게이트웨이는 HTTPRoute를 살펴 적절한 InferencePool 백엔드를 인식

2. Endpoint Selection

    - 게이트웨이는 가능한 pod로 단순히 포워딩하는것이 아닌, 인퍼런스 특화된 확장된 라우팅을 수행

    - 가용 파드 중 파드의 메트릭을 조사해 최선의 파드로 라우팅함.

        - 큐 길이, GPU 메모리 사용량, 로드된 모델 버전 및 어댑터 등을 고려

3. Inference-Aware Scheduling

    - 선택된 파드는 요청을 가장 짧은 지연시간이나 가장 큰 효율성으로 요청을 처리함

        - 유저의 중요성이나 자원의 필요에 따라 결정됨

    - 게이트웨이는 이후 트래픽을 특정 파드로 포워딩함.

- 추가적인 단계는 모델을 고려한 지능형 라우팅 메카니즘을 제공한다.

    - 여전히 클라이언트는 하나의 요청처럼 느낄 수 있다.

- 디자인은 확장 가능하여, 어떤 인퍼런스 게이트웨이도 추가적인 라우팅 전략, 발전한 스케줄링 로직, 특수한 하드웨어 요구를 지원할 수 있도록 함.

# Benchmarks

- vLLM 기반의 모델 서빙을 표준 쿠버네티스 서비스에 배포하여 평가했다.

- H100 GPU 여러개를 탑재한 노드에 Llama 모델을 띄운 10개의 vLLM 파드를 실행했다.

- Latency Profile Generator 툴로 트래픽을 생성하고, 처리량과 지연시간, 다른 메트릭을 계산했다.

- 워크로드로는 ShareGPT 데이터셋을 사용했고, 100에서 1000QPS로 상승되었다.

## Key Results

[image](https://app.capacities.io/d1cf29da-a5be-492d-839a-e228b21e0129/b00a40c3-00e0-4a20-8226-27a089877447)

- 낮은 레이턴시

    - 출력 토큰당 레이턴시

        - ESE는 500 이상의 높은 QPS에서 더 낮은 레이턴시를 보였다.

        - 모델을 고려한 라우팅 선택이 GPU 자원을 효율적으로 사용하도록 하며 대기열을 줄이고 메모리 사용률을 높였다.

    - P90 레이턴시 전반

        - 비슷한 트렌드가 드러나지만, ESE는 E2E tail 레이턴시를 낮추었다.

            - 트래픽이 400-500 QPS일 때 가장 크게 드러난다.

- 모델 인지 라우팅이 GPU 기반의 LLM 워크로드의 레이턴시를 낮추는 것이 드러났다.

    - 가장 로드가 적고 성능이 좋은 모델 서버를 선택한다.

    - 이를 통해 전통 로드밸런싱에서 크고 긴 인퍼런싱 요청을 할때 나타나는 핫스팟을 피할 수 있었다.

# Roadmap

- GA에 도달하였고, 다음과 같은 기능을 계획하고 있다.

    - 리모트 캐시 히트율을 높이기 위한 위한 접두어 기반 로드밸런싱

    - 자동 롤아웃을 위한 LoRA 어댑터 파이프라인

    - 동일한 중요도의 워크로드에 대한 공평한 우선권 제공

    - 종합, 모델 수치를 기반으로 한 수평 파드 스케일링 지원

    - 큰 멀티모달 입출력 지원

    - 모델 타입 추가

    - 다양한 액셀레이터 지원 (지연시간과 비용을 고려한 여러 개의 엑셀레이터를 지원하여 로드밸런싱)

    - 독립적으로 스케일링되는 풀에 대한 분리된 서빙

# Summary

- 모델 서빙과 쿠버네티스 내부의 툴을 일관되게 제공하여, AI/ML 모델 라우팅을 단순화하고 표준화하는 것이 목표

- 모델 인지 라우팅, 중요도 기반 우선순위 결정 등을 통해 ops 팀이 적절한 LLM 서비스를 적절한 유저에게 자연스럽고 효율적으로 제공할 수 있도록 한다.

