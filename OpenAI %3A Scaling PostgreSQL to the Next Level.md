- 출처 : [OpenAI: Scaling PostgreSQL to the Next Level](https://www.pixelstech.net/article/1747708863-openai%3a-scaling-postgresql-to-the-next-level)
- Sharding & Off-loading
	- Sharding은 row 단위 분할
		- key를 기반으로 row들을 나누어서 저장
		- 파티셔닝!
	- off-loading은 부하를 레플리카를 통해 해결하는 방식
- 오픈AI는 샤딩을 하지 않고, 읽기 레플리카만 있는 클래식 PG 아키텍처로 운영된다.
	- 읽기 확장성은 문제가 없는데, 문제는 쓰기 요청이 큰 보틀넥이다.
		- 이러한 과정에서 쓰기 요청을 최대한 최적화하여 해결한다.
			- 쓰기 로드를 가능하다면 오프로딩으로 해결
			- 새로운 서비스를 프라이머리 데이터베이스에 추가하는것을 피함
- PG의 MVCC 디자인은 하기한듯이 몇가지 이슈가 있고, 이러한 과정에서 최적화를 이루어내어야 한다.
	- 테이블/인덱스 bloat, 복잡한 Vacuuming, 모든 쓰기 요청이 새로운 버전을 생성함, 인덱스 접근이 visivility check를 필요로 할 수 있음
	- WAL이 증가하면 Replication lag이 증가하는 등의 문제가 발생하고, 레플리카가 늘어나면 네트워크 대역폭이 보틀넥이 될 수도 있다.
- 다음과 같은 방법으로 이슈를 해결한다.
	- 프라이머리 데이터베이스 로드
		- 가능한 모든 쓰기 요청을 오버로딩한다.
		- 어플리케이션 레벨에서의 쓰기 요청 회피
		- Write 요청이 많은 상황에서 이를 줄이기 위한 Lazy write
			- 중요하지 않은 쓰기 작업을 모았다가 나중에 일괄 처리한다.
		- 데이터 백필중 주기 컨트롤
		- 추가로, 최대한 많은 읽기 요청을 레플리카로 보내고, 프라이머리로 가야하는 경우는 최대한 효율적인 방법으로 처리하도록 한다.
	- 쿼리 최적화
		- 긴 트랜젝션은 가비지 컬렉션과 자원 소모를 방해할 수 있어 Idle in Transaction에 타임아웃을 건다.
			- session, statement, client 레벨에 타임아웃을 설정한다.
		- ORM은 N+1쿼리와 같이 비효율적인 접근 패턴과 쿼리로 이어질 수 있기 때문에 주의를 필요로 한다.
	- Addressing Single Points of Failure
		- 프라이머리가 죽으면 쓰기 요청을 수행할 수 없다.
		- 반대로 많은 중요한 요청이 읽기만 발생하므로, 프라이머리가 죽어도 읽기는 수행할 수 있다.
		- 읽기 레플리카에 그룸을 설정하고 나눈 관리한다.
			- 요청에 우선순위를 설정하여 우선순위가 높은 요청과 낮은 요청을 다른 그룹으로 보내어 영향받지 않도록 한다
	- Schema Management
		- 스키마 또한 클러스터에서 경량화의 대상이다.
		- 새로운 테이블을 생성하고 워크로드를 추가하는것은 금지
		- 컬럼 추가 / 제거는 괜찮지만, full table rewrite를 수행하는 동작은 금지
		- 인덱스 추가, 삭제는 괜찮지만 CONCURRENTLY를 의무적으로 사용
		- 1초 이상 작동하는 긴 쿼리는 스키마 면경을 블락할 수 있어, 이또한 최적화를 하여 블록되지 않도록 한다.
	- 결과
		- 클러스터 전반에서 OpenAI의 크리티컬 서비스에 대해 1M QPS를 핸들링할 수 있도록 한다.
		- Replication Lag이 높아지지 않고 레플리카를 추가할 수 있다.
		- 낮은 레이턴시를 유지하면서 읽기 전용 레플리카를 지역별로 추가할 수 있었다.
		- 지난 9달간 PG 운영하면서 SEVo 단 하나
		- 이후의 확장을 위해 ample capacity를 확보했다.
- 오픈AI가 원하는 대부분의 기능이 있고, 이는 대부분 이미 PG커뮤니티에서 기존에  다루어졌지만 Azure의 관리형 서비스를 사용하여 활성화 할 수 없는 기능이었다.
	- Disable Index
		- 대규모 데이터 로딩에서 인덱스 비활성화로 쓰기 성능을 활대하기 위해 필요
		- pg_index_system에서 indisvalid를 false로 설정하면 되는데, Azure에서 슈퍼유저 안되서 못쓴다.
	- pg_stat_statement의 기능 부족
		- P95, P99는 메모리 접근하는게 많아져서 구현할 일 없다.
		- pg_stat_monitor 익스텐션은 percentile latency를 제공하고 (오버헤드가 발생), eBPF를 써도 된다.
	- 스키마 추가 / 삭제 등 DDL에 대한 히스토리 기록
		- log_statement를 DDL로 설정하면 모든 DDL 명령이 로깅된다.
		- Datadog 모니터링 - PgBouncer 커넥션 풀 사용하는데, SQL로 쿼리할 수 있는 IaC에서 스키마 변경을 추적하기 위해 DDL에 대한 시스템 뷰를 원해서 필요한 것이 좀 달랐다.
	- Semantics of Monitoring views
		- State=Active, wait_event=ClientRead가 두시간 넘게 이어지고, idle_in_transaction 타임아웃도 작동 안했었다.
		- PG가 명령을 아직 안끝났다고 여기고, 사용중이라고 여기는 것이다.
			- ClientRead는 프로세스가 클라이언트의 요청을 기다리는 건데, 보통은 COPY FROM STDIN이다.
			- TCP blocking이나 BIND와 EXECUTE 사이에 어딘가에서 stuck된걸 수도 있다.
			- 이걸 "버그"라고 확실히 말하기는 어렵다.
				- CPU 입장에선 idle일 수 있는데, CPU에서 안돌고있거나 client 입력 기다리면서 루프 돌 수 있다.
			- Pigsty는 HAProxy로 접근해서, 프라이머리 서비스는 LB레벨에서 커넥션 수명을 설정할 수 있다.
	- 현재의 보수적인 Config 파라미터
		- 여전히 기본 메모리가 256MB로 설정되어 있다.
		- 기본값이 보수적인건 나쁘지 않은데, 동적 Configuration을 더 확대하여 해결할 수 있다.
		- 최초 파라미터 튜닝을 위한 툴을 제공하는 것도 가능하다.