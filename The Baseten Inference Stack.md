---
출처: '[The Baseten Inference Stack](https://www.baseten.co/resources/guide/the-baseten-inference-stack/#the-challenge-of-building-fast-cost-efficient-and-reliable-inference)'
---

# The challenge of building fast, cost-effecient and reliable inference

- 배포 환경에서 AI 모델을 운영하는 것은 데모랑 다르다.

    - 지연시간, 업타임, 비용에서 추가적으로 엄격한 요구사항들이 추가된다.

    - 요구사항들이 만족되지 않는다면, 어플리케이션이 느리거나 신뢰도 떨어지게 느껴지고, 엔드 유저의 고통으로 이어진다.

- vLLM 등의 인퍼런스 프레임워크와 GPU를 구동시키는 것은 나쁘지 않은 성능을 보인다.

    - 나쁘지 않은에서 훌륭함으로 이동하는 것은 어렵다.

    - 훌륭한 서빙은 로드가 걸릴때 낮으면서 예측 가능한 레이턴시, 일정 규모에서 합리적인 비용으로 서빙하는 것

- 이러한 간극을 메꾸기 위해 인퍼런스의 모든 레이어를 캘리브레이션 해야 한다.

    - 모델부터 하드웨어와 그 사이의 다양한 소프트웨어 계층까지 모두 튜닝해야 한다.

    - Baseten 인퍼런스 스택은 이러한 최적화를 단일 플랫폼으로 제공한다.

        - 최고의 오픈소스와 내부적인 추가 개선으로 제공한다.

        - 모델과 Baseten으로 배포되는 모든 시스템은 기본적으로 이러한 이득을 얻는다.

# Why building performant inference at scale is challenging

- 프로덕션에서 Performant 인퍼런스는 낮은 레이턴시, 모델 성능, 높은 처리량, 최대 신뢰도를 의미한다.

- 제품이 즉각적이다라고 느껴질 수 있음을 보장할 정도로 인퍼런스가 빨라야한다.

    - 코드 완성 툴이든 문장 기반 어플리케이션이든, 첫 바이트부터 레이턴시 전반까지의 모델 성능을 최적화하는 것은 엔드 유저 경험에 영향을 미친다.

    - 인퍼런스는 워크로드의 복잡성, 입력 트래픽의 다양성과 규모, CP의 신뢰도등의 모든 환경과 타협하지 않고 빨라야 한다.

- 새로운 성능 연구는 엄청나게 빠른 속도로 발표되고 있고, 보통 몇주 뒤면 배포 환경에서 사용된다.

    - VRAM의 KV 캐시를 줄이는 Multi-head Latent Attention이 Deepseek에서 나온 것 처럼 움직인다.

    - 새로운 모델이 나오면 이러한 테크닉을 출시 첫날부터 베포환경에 제공할 수 있어야 한다.

    - SOTA 성능급 모델의 새로운 테크닉을 항상 따라가는 것은 불가능에 가깝다.

        - 신뢰성이나 성능 하락 적은 비용 효율성은 연구팀과 분산 인프라 팀에 불가능에 가깝다.

- 인퍼런스의 속도를 측정하기 위해 많은 메트릭이 사용된다

    - 요청 레이턴시, 첫 토큰 시간, 첫 바이트 시간, 토큰간 레이턴시, 초단 토큰 생성 개수 등

    - 이러한 지표들은 각기 다른 장단점을 가지므로, 특정 유스케이스에 맞춰 성능을 종합적으로 이해하는 것이 중요하다.

- 인퍼런스를 빠르게 하기 위해, 호출한 지점에서 위치가 가깝도록 전략적으로 설계해야 한다.

    - 또한 최신 모델 연구에 사용된 기술들을 적용할 수 있어야 하며, 이러한 기술들을 포함하나 이 내용에만 제한되선 안된다

        - 저레벨 최적화 : 커널 퓨전, 메모리 계층 최적화, 어텐션 커널, 비동키 연산, PDL 등 

        - Speculation 전략 : 큰 접두사의 재연산을 막기 위한 KV-cache 재활용과 오프로딩

        - 비집계 서빙 : 서로 다른 런타임의 분리된 하드웨어로 서빙하기 위한 스케일, prefiill, 디코딩

        - 학습 후 부동 소수점에 대해 양자화 : 무시할 수 있는 복잡도를 무시하여 속도 향상

        - 위치적으로 가까운 GPU서버로 라우팅하여 KV와 LoRA 캐시 사용률 향상

    - 각 최적화는 강력하지만, 문제의 일부 부분만 하므로 배포에서 결합되었을 때 더 강력해진다.

        - KV 캐시 재사용은 TTFT를 개선하지만 토큰간 레이턴시에는 관련이 적다.

- 높은 가용성은 필요하지만 신뢰도높은 인퍼런스 서비스의 충분조건은 아니다.

    - 다운타임을 피하는 것은 필수적이며, 유저가 항상 접근 가능하며 일관된 성능을 보장해야 한다..

    - 모델의 출력은 지속적으로 낮은 레이턴시를 제공해야 한다.

- 많은 SLA는 p90이나 p99 레이턴시에 기반하지만, 충분하지 않다.

    - p99 지연 시간 외의 이상치(outlier)가 전체 사용자 경험을 저하시킬 수 있기 때문이다.

- 신뢰도 높은 메트릭은 다음과 같은 구성을 포함한 강건한 인프라 설계를 필요로 한다.

    - 트래픽 스파이크에 대응하기 위한 오토스케일링

    - Active-Active Reliability와 크로스 클라우드 오케스트레이션

        - 노드나 AZ, 리전, CP가 죽어도 작동해야 한다.

    - 강력한 alert과 자동 실패 핸들링

        - 다운스트림 하드웨어, 네트워크, 드라이버 오류에 대응할 수 있도록 한다

    - 여러 프로토콜(HTTPS, WebSocket, gRPC)에 대한 적절한 핸들링

        - 모든 모달리티와 트래픽 패턴을 지원할 수 있어야 한다.

    - 이러한 인프라를 구성하는 것은 CSP간의 차이를 줄이기 위한 지속적인 추상화를 필요로 한다.

- 무한한 자원을 갖고있으면 트래픽이 커져도 안전하도록 초과 배포하면 되는데, 비싸며 자원 낭비이다.

    - 필요 없는 초과 배포 없이 충분한 헤드룸을 보장하기 위해 Utilization이 중요한 메트릭이다.

    - 인퍼런스는 배포 환경에서 다음을 만족하면 효율적이다.

        - 요구 사항에 맞는 하드웨어에 모델을 배포할 때

        - 서로 다른 모델을 독립적으로 필요에 따라 스케일링할 때

        - 적절한 자원에 접근하여 신뢰성 높은 스케일링을 하도록 한다.

        - 0에서부터도 입력 요청을 안전하게 큐잉하면서 빠르게 스케일링 할 수 있어야 한다.

- GPU가 복합적인 문제다.

    - B200같은 최신 GPU는 CSP에서 오래 걸리는 예약을 해야한다.

        - 하이브리드 인프라의 유연성을 채택하면 추가적인 컴퓨팅 자원으로 확장하면서 비용 효율성과 높은 업타임, 성능을 유지할 수 있다.

# Key requirements of a production-ready inference service

- 프로덕션 수준을 위해, 인퍼런스 서비스는 통합된 접근을 사용해야 한다.

    - 프로덕션 레벨의 추론 서비스를 구축하기 위해서는 인프라 구성요소부터 런타임의 밀리초 단위 구현 세부사항까지 통합된 접근 방식이 필요하다.

- Baseten에서 인퍼런스 스택은 확장성, 클라우드 호환성, 장애 저항성 인프라를 구축하면서 모든 모델에 대해 좋은 성능의 런타임을 제공하여 결합된 개발자 경험으로 제공한다.

    - 인퍼런스 스택은 다음과 같은 작업을 할 수 있다.

        - 낮은 레이턴시의 모델 인퍼런스를 이용한 실시간 AI 어플리케이션 설계

        - 유연한 능력과 경제성을 위한 멀티 클라우드 환경 컴퓨팅

        - 멀티클라우드 / 멀티리전에서의 99.99% 이상의 uptime과 꾸준한 성능

        - 파일 기반 설정과 프로그래밍 기반 컨트롤을 통한 인퍼런스 인프라에 대한 완전한 제어

# Baseten Inference Stack: Combining infracstructure and runtime optimization

- Baseten 인퍼런스 스택은 2개의 결합된 레이어로 구성된다.

    - 인퍼런스 런타임

        - 모델이 실제 어떻게 작동하는지 정의한다.

        - 런타임 최적화, 커스텀 커널, speculation 엔진등이 포함된다.

    - 인퍼런스 최적화 인프라

        - 엔드 유저가 적절한 자원에 신뢰성 있게 접근할 수 있도록 보장하는 레이어

        - 요청 라우팅, 오토스케일링, 멀티클라우드 관리를 제공한다.

- 런타임 최적화는 인퍼런스 인프라 위에 올라가고, 성능 차별화의 핵심이다.

    - 커스텀 커널 최적화, 디코딩 전략, 모달리티 특화 런타임, 성능 개선을 위해 선택된 기능들을 사용한다.

- AI 모델 인퍼런스는 GPU에서 실행되고, 커널은 GPU 실행의 바닥 블락이다.

    - 각 커널은 특정한 연산을 수행하는 로우레벨 코드이다.

    - 커널 성능이 나쁘면 전반적인 인퍼런스 성능이 느려진다. 특히 큰 처리량이 있을 때.

    - 커스텀 커널 최적화를 사용하고, 특정 모델과 워크로드를 작동하는 하드웨어에 대해 최적화하기 위해 SOTA 인퍼런스 프레임워크 중에서 선택한다.

    - 다음과 같은 테크닉을 사용한다.

        - 커널 퓨전

            - 여러 연산을 하나의 GPU 커널로 조합하여 오버헤드를 감소시킨다.

            - 연산 사이의 메모리 읽기/쓰기를 최소화하고, 커널 실행 오버로드를 줄인다.

        - 메모리 계층구조 최적화

            - 텐서가 GPU 메모리의 가장 효율적인 부분에서 접근되고 저장되도록 보장한다.

            - 레지스터와 공유 메모리를 글로벌 메모리보다 우선한다.

        - 어텐션 커널

            - 워크로드에 특화된 어텐션 커널로 접근 속도, 메모리 풋프린트, 컨텍스트 길이와 확장성을 최적화한다.

        - 비동기 연산과 PDL

            - 동시성 실행을 허용하고 Hopper와 Blackwell 아키텍처에서 지원하는 커널 실행 패턴을 사용해 GPU 사용률을 향상시킨다.

- 다음과 같은 과정으로 실행된다.

    1. Tensor-RT, SGLang, vLLM같은 인퍼런스 프레임워크를 테스트해서 워크로드 성격과 타겟 하드웨어에 대해 성능이 가장 좋은 프레임워크를 선택한다.

    2. 프레임워크가 제공하는 것을 확인하고, 버그를 수정하고 기능을 확장하여 특수화된 인퍼런스 워크로드를 지원하도록 한다.

        - 최적화 커널은 초당 토큰 수를 더 높이며, 첫 토큰 생성에 걸리는 시간을 줄이고, 전반적인 성능 향상을 제공한다.

- 토큰이 자기 회귀적으로 생성되는 디코딩 단계는 일반적으로 인퍼런스에서 가장 비싸고 느린 과정이다.

    - 디코드는 토큰을 하나씩 생성하고, VRAM의 대역폭에서 병목이 발생한다.

### Spaculative Decoding

- 포워드 경로에서 여러 개의 토큰을 만들면 어떨까? 라는 일반적인 아이디어에 대한 넓은 범위의 전략

- 토큰간 지연 시간(ITL)을 크게 개선하여, 사용자에게 낮은 지연시간과 높은 TPS를 제공한다.

- 전통적인 접근은 드래프트 타겟 시스템이었다.

    - 더 작은 드래프트 모델이 몇 토큰 뒤를 예측하고, 풀 사이즈의 타겟 모델이 예측을 검증한다.

    - 검증이 생성보다 저렴하기 떄문에, 드래프트 모델의 성능에 따라 이러한 전략이 TPS를 향상 시킬 수 있었다.

- 다양한 모델에서 성능이 프롬프트, 토픽마다 다르다.

    - 다양한 speculation 전략을 사용하고, 라이브 트래픽에 따라 동적으로 파라미터를 조정한다.

        - Eagle 3: 현재 가장 진보된 드래프트 모델

        - Lookahead decoding: 이전 컨택스트에 기반해 다음 토큰을 예측한다.

        - Medusa: 타겟 모델에 파인튜닝과 추가적인 디코드 헤드를 접목한 자기 추측에 근거한 전략

        - 드래프트 기반 디코딩: 특정 유즈케이스에 최적화된 타겟 특화 추측 디코딩

- 배치사이즈가 작고 남는 GPU 연산이 있을때 유용하다.

    - 높은 배치사이즈에서는 검증이 연산보다 비용이 커지기 떄문에 동적으로 비활성화된다.

### Quantization

- 훈련 정확도로 인해 대부분의 AI 모델은 FP16이나 BFLOAT16으로 훈련된다.

    - FP16은 모델의 각 파라미터가 VRAM에 저장과 로드할때 2바이트를 차지하고, 16비트 표현을 위해 설계된 텐서 코어에서 연산된다.

- GPU는 다른 양자화도 지원한다.

    - Lovelace와 Hopper GPU는 FP8 지원을, Blackwell은 FP4 지원을 추가했다.

    - 이러한 포맷은 이전 세대의 정수 기반 포맷보다 높은 다이나믹 레인지를 제공한다.

- 훈련 후 양자화는 모델 weight와 선택적으로 KV cache등의 다른 값의 정확도도 달라진다.

- 양자화된 모델은 저장에 적은 VRAM을 사용하며, 디코딩에 적은 대역폭을 필요로 하고, 더 높은 연산 속도를 보인다.

- 모델 출력 퀄리티에 영향을 미칠 수 있어, 이 리스크를 최소화하기 위해 다음과 같은 전략을 사용한다.

    - 기본 FP 포맷을 다이나믹 레인지가 큰 FP8이나 FP4로 사용하여, 모델의 퀄리티를 지킨다.

    - 모델과 GPU 타입에 따라 최적의 양자화 스키마를 선택한다.

    - 양자화 후 모델을 벤치마크하여 perplexity를 보장하고, 점수차가 무시 가능한정도인지 확인한다.

    - 현실세계의 인퍼런스 샘플로 벤치마크를 확인한다.

- Baseten에서 모델 양자화는 항상 선택적이며, 유저의 허가 없이 양자화 하지 않는다.

### KV-cache

- LLM에서 인퍼런스 시간의 많은 부분이 생성 시작 전에 프롬프트를 처리하는 과정에서 소모된다.

- KV 캐시는 이러한 prefill state를 저장하여, 비슷한 컨택스트의 연속적 요청에서 재사용한다.

    - 시스템 프롬프트나 코드베이스 전반을 저장한다

- Baseten은 KV 캐시의 유용성을 개선하기 위한 다양한 방법을 사용한다.

    - KV캐시 인지 라우팅 : 요청을 필요한 컨택스트가 캐싱된 레플리카로 자동으로 라우팅한다.

    - KV 캐시 가용성 : 잘 알려진 "You are a helpful assistant"같은 프롬프트를 재사용한다.

    - CPU 오프로딩 : 사용하지 않는 캐시 블락을 CPU 램으로 옮겨 VRAM 사용을 줄인다.

    - 디스크 기반 캐싱: InfiniBand로 캐시 오프로드를 노드간 분산처리한다.

### Parallelism

- 큰 모델을 여러 GPU와 멀티 노드에서 실행할 때, 텐서 병렬화와 expert 병렬화같은 병렬화 전략이 통신 오버헤드를 최소화한다.

    - TP와 EP에 다른 병렬화 기법을 사용하여 런타임에서 큰 모델을 효율적으로 작동시킨다.

### Request prioritization

- prefill을 분리하여 다른 하드웨어에서 디코드하기 위해 분할 서빙을 사용하지 않는 워크로드에서도 런타임 레이어에서 TTFT를 개선하기 위해 사용하는 방법

- baseten 런타임은 디코드보다 prefill 단계를 우선하여, 낮은 TTFT의 속도를 개선한다.

    - 새로운 요청이 들어오면 prefill 우선으로 연산을 수행한다.

    - Prefill을 배치 내의 다른 요청의 decode보다 우선 연산하여 대역폭 제한 디코딩에서 ITL에 영향을 최소화하며 TTFT를 낮게 유지할 수 있다.

        - 특히 KV 캐시를 재사용하는 경우 유리하다

### Batch Processing

- 전통적인 배치는 처리 이전에 요청이 도달하기를 기다려야 하고, 이는 받아들일 수 없는 정도의 레이턴시로 이어졌다.

- Baseten은 업계 표준인 연속적인 토큰 레벨 배치를 사용한다.

    - 모든 요청을 기다리는 대신, 토큰 레벨에서 배치를 처리한다.

    - 요청은 처리중인 배치에 스트림으로 입력될 수 있다.

    - 레이턴시는 보장된 처리량으로 최소화된다.

    - 요청을 대기하면서 발생하는 idle 시간이 없다.

- 높은 GPU 사용률과 개선된 평균 응답 시간으로 이어지고, 트래픽 스파이크에서 더 크게 작용된다.

### Structured Outputs

- 정의된 스키마로 응답을 반환하는 것은 엔터프라이즈 유즈케이스에서 필수적이다.

- 프로그래밍적 상호작용과 API와의 호환성을 가능하도록 하며, 툴 사용을 지원하여 에이전틱 워크플로우를 가능하도록 한다.

- LLM은 구조화된 출력을 반환하도록 프롬프트를 작성할 수 있지만, 신뢰도가 떨어진다.

    - Baseten 런타임은 디코딩 과정에서 생성될 토큰의 확률을 미리 정의된 스키마에 맞추어 조정하여 100% 스키마 준수를 보장한다.

    - 토큰 간 레이턴시에 감소 없이 스키마 고수를 보장한다.

        - 에이전트와 툴 콜링이 LLM으로부터 항상 valid한 출력을 받는다.

### Performance

- AI 엔지니어가 모델 퍼포먼스에 관해 얘기할때, 기본적으로 LLM 인퍼런스에 대해 얘기한다.

    - KV캐싱부터 specualtive 디코딩까지 대부분의 중요한 테크닉은 LLM 인퍼런스를 생각하고 개발한다.

- LLM뿐만 아닌 다른 모달리티의 생성형 AI도 빠른 프로덕션 인퍼런스로 이점을 얻을 수 있다.

    - Llama, Qwen, DeepSeek등의 LLM 모델

    - Whisper등의 Auto Speech Recognition (ASR) 모델

    - Orpheus TTS 등의 음성 합성 모델

    - Nomic Embed Code 등의 임베딩, 리랭킹, 분류, 리워드 모델

    - 이미지, 비디오 생성 모델

- LLM이 아닌 TTS, 임베딩같은 모델도 LLM 백본을 갖고 있다는 것이 모달 특정 접근의 중요한 인사이트다.

    - LLM 백본이 아닌 자동회귀 트랜스포머 기반 모델의 경우도 비슷한 최적화 기법과 테크닉으로 최적화할 수 있다.

    - diffusion 기반 이미지 / 비디오 모델은 완전히 다른 접근을 필요로 한다.

- Baseten 스택은 LLM을 고려하여 설계되었다.

    - LLM은 인퍼런스 워크로드의 대부분을 차지하고, 가장 큰 워크로드를 차지한다.

    - 인퍼런스 최적화 인프라와 런타임의 거의 모든 테크닉과 기능이 활용되어 최적화될 수 있다.

        - 100기가가 넘는 LLM은 콜드스타트 최적화와 활성화-활성화 신뢰도 등의 인프라 레이어에서 이점을 얻는다

        - 작동중일때, 지능형 요청 라우팅은 레플리카간의 로드를 밸런싱하면서 KV캐시와 LoRA 캐시의 히트레이트를 높일 수 있다.

        - 가장 큰 모델은 멀티 노드 인퍼런스와 분할 서빙이 필요하고, 인프라 레이어에서 제공된다.

    - 런타임 레이어는 대규모 배포환경에서 가장 큰 LLM을 낮은 지연시간과 높은 처리량으로 처리하도[록 디자인되었다.

        - KV 캐시 재사용과 요청 최적화는 큰 입력에 대해 빠른 TTFT를 제공한다.

        - Windowed attention은 모델이 긴 컨택스트를 지원할 수 있도록 확장한다.

        - 양자화와 speculation 엔진은 낮은 인터토큰 레이턴시를 제공하며, 유저의 TPS를 높게 유지한다.

- Speech To Text

    - STT 모델은 트랜스크립션 모델의 기반이고, 딕테이션은 목소리 파이프라인의 필수 컴포넌트이다.

    - 이러한 모달리티의 성능 평가 지표는 다음과 같다.

        - 속도 펙터

            - 트랜스크립션의 생성되는게 실시간보다 몇배 빠르게 되었는지 (Whisper 1000 달성)

            - 팟캐스트 트랜스크립션등 유저가 큰 파일을 올리는 유즈 케이스와 관련된다.

        - 라운드트립 레이턴시

            - 단일 청크의 오디오가 얼마나 빠르게 트랜스크립트되어 반환되는지 (Whisper 기준 200ms 이하)

            - dictation같은 실시간 유즈 케이스에 사용한다.

        - 퀄리티

            - Word Error Rate로 측정. 배포 환경에 정확한 트랜스크립션을 측정

- Text To Speech

    - 스피치 파이프라인의 다른면에는 TTS 모델이 보이스 에이전트의 뒤에 존재한다.

    - TTS 모델에 대해 주 레이턴시 메트릭은 TTFB 혹은 첫바이트 오디오 생성에 걸린 시간이다.

    - 실시간 대화 에이전트 등의 온라인 유즈 케이스에 유용하다.

    - 주 처리량 메트릭은 동시 처리 스트림 개수, GPU당 처리 가능 실시간 유저 수이다.

    - Orpheus TTS같은 음성 합성 모델은 LLM 백본으로 설계되었다.

        - LLM 툴링을 적용하여 웹소켓같은 스트리밍 프로토콜에 결합하여 TTS 모델에 대한 업계 최고의 레이턴시, 출력량을 제공한다.

- Embedding

    - 효율적인 임베딩 인퍼런스는 높은 출력의 배치 프로세싱과, 낮은 레이턴시의 단일 쿼리 인퍼런스 실시간을 모두 지원해야 한다.

        - 하나를 최적화하는 것은 다른쪽의 성능을 하락시킨다.

        - 큰 토큰 비력은 OOM 에러로 이어질 수 있고, 작은 배치사이즈는 GPU의 처리량을 끝까지 못쓰며, 전통적인 런타임은 다양한 워크로드들 사이의 성능을 밸런싱하기 어렵다.

    - Baseten 임베딩 인퍼런스는 이러한 요구를 대응하기 위해 만들어졌다.

        - 코어에는 요청 수가 아닌 시퀀스 입력에 따라 동적으로 토큰화된 입력을 패킹하여 레이턴시와 처리량 균형을 맞추도록 설계된 텐서RT-LLM 기반 엔진이 사용되었다.

            - OOM을 트리거하지 않고 GPU 사용률을 높일 수 있다.

        - 추가적인 성능 향상을 위해, 양자화를 도입하여 정확도를 유지하며 메모리 필요량을 줄였다.

        - fused transformer 레이어를 사용하고 어텐션 커널을 최적화하여 메모리 대역폭 사용량을 최소화하고 연산 오버헤드를 줄였다.

        - 베치 중심 워크로드와 밀리세컨드 단위 상호작용 어플리케이션에 모두 적절하게 제작되었다.

        - 이를 통해 이전의 시장 리드 솔루션보다 2배 많은 처리량과 10% 낮은 레이턴시를 달성했다.

- Image / Video

    - 이미지와 비디오 생성 모델은 다른 모달리티와는 다르게 런타임에 diffusor를 필요로 하여, 다른 런타임을 필요로 한다.

    - 이러한 모델의 주 성능 지표는 이미지나 비디오 생성의 엔드투엔드 요청 레이턴시이다.

    - Diffuser는 토치 컴파일, 효율적인 어텐션, 훈련 후 양자화 등에 대해 자체적인 커스텀 커널을 필요로 한다.

        - 대부분의 이미지 모델 인퍼런스의 장점은 로우레벨 CUDA 작업에서 온다.

    - 이미지와 비디오 모델의 특이한 점 중 하나는 LLM보다 더 많은 자유도를 갖고 큰 속도 향상을 위해 작은 퀄리티 하락을 타협한다는 점이다.

        - distillation과 같은 전통적 접근에서 few-step 인퍼런스의 latent consistency같은 테크닉은 큰 성능향상으로 이어질 수 있다.

        - 프롬프트 레벨의 다양한 조정으로 속도와 퀄리티를 동적으로 설정할 수 있다.

            - 이미지 해상도부터 디퓨전 스텝 개수 등의 펙터가 있다.

### Load Balancing

- Baseten 런타임은 유명한 오픈소스 인퍼런스 프레임워크와 커스텀 서버에서의 컴파일과 최신 커널 최적화를 지원한다.

    - Baseten LB는 KV 캐시와 모델 어댑터 기반으로 라우팅해 지연시간과 퀄리티 목표를 달성한다.

        - 요청의 위치에서 가까운 하드웨어로의 서빙 요청, 메모리에 유용한 정보를 이미 저장하는 경우는 인퍼런스 속도에 큰 차이를 만든다.

        - 지능형 라우팅을 위해 KV-캐시와 LoRA 인식 기반의 위치 인지 로드밸런싱을 수행한다.

- 위치 인지 로드밸런싱은 글로벌 배포되었을 때 레이턴시에 민감한 워크로드에 중요하다.

    - 위치 인지가 이루어지지 않으면 많은 네트워크를 타야하고, 레이턴시가 길어진다.

- KV 캐시는 인퍼런스동안 어텐션 레이어로부터 키-밸류 텐서를 저장한다.

    - 새로운 토큰에 대해 과거 컨택스트를 재연산하는 대신 캐싱된 데이터를 재사용하여 다음 토큰을 빠르게 생성한다.

    - KV캐시 인지 라우팅은 관련된 컨텍스트를 이미 갖고있는 레플리카로 요청을 보내도록 한다.

        - 10개의 다른 레플리카가 있다면, 요청을 비슷한 요청이 있었던 레플리카로 보내 캐시히트 확률을 높인다.

    - 챗봇과 유사한 워크로드에서 prefill 레이턴시를 크게 줄인다.

- LoRA 인지 라우팅은 요청을 특정 LoRA를 메모리로 로드한 레플리카로 전송하는 방식이다.

    - 이미지 생성과 LLM의 유즈 케이스에 관련된다.

    - 일부 부분집합만 메모리에 들어갈 수도 있다.

        - LoRA 인지 라우팅은 각 요청이 적절한 어댑터가 로드된 레플리카로 요청이 들어가도록 한다.

        - 아니면, 레플리카는 가장 연관된 LoRA를 다운로드받아 요청을 처리해야 한다.

- 다른 워크로드에 대해 서로 다른 프로토콜이 적절할 수 있다.

    - 일반적인 HTTP 요청 - 응답 패러다임에 추가로 웹소켓과 gRPC를 지원하여 성능을 향상한다.

    - 새로운 커넥션마다 요청에서 오픈-클로징 오버헤드가 발생하는 HTTP와 대조적으로 웹소켓은 클라이언트-서버 간 연속적인 커넥션을 제공한다.

        - 구조화되지 않은 실시간 데이터를 전송할 때 유연하다. 서버는 요청을 받아 파싱하고 다운스트림으로 처리할 수 있다.

        - AI 전화같은 실시간 스트리밍을 요구하는 유즈케이스에 유용하다.

    - 웹소켓과 유사하게 gRPC는 구조화된 데이터에 대해 양방향 스트리밍을 지원한다.

        - gRPC로 전달된 요청은 미리 정의된 스키마를 따라 입력 파싱 비용을 없앤다.

        - 추가적인 검증 레이어는 gRPC를 웹소켓보다 조금 느리게하지만, 요청 상의 번역이 이루어져야 하는 (text to video 등) 유즈케이스에서 유용하다.

### Auto Scaling

- 성능 좋은 오토스케일링은 다음과 같은 조건이 굉장히 중요하다.

    - 트래픽의 스파이크에서 얼마나 빠르게 스케일할 수 있는가

        - 새로운 모델 레플리카가 배포되는데 5분이 걸리는것과 30초가 걸리는 것은 P99, P90 레이턴시에서 큰 차이가 난다.

        - 인하우스 콜드스타트 최적화와 최적 성능을 위한 파라미터 튜닝을 사용하여 고객의 타겟 SLA를 달성한다.

    - 트래픽이 줄어 하드웨어 사용량이 감소했을 때 얼마나 비용 효율적으로 작동시킬 수 있는가?

- 레이턴시와 비용 효율성 목표를 달성하기 위해 오토스케일을 잘 설정해야 하며, 다음과 같은 파라미터를 설정해야한다.

    - 최소 / 최대 레플리카 개수 (활성화된 레플리카의 최소 / 최대 개수)

    - 오토스케일링 윈도우 (스케일 확장/축소 결정을 위해 트래픽을 분석하는 시간 범위)

    - 스케일 다운 딜레이 (사용되지 않는 레플리카가 삭제되는데 걸리는 속도)

    - 동시성 타겟 (스케일링 이전에 레플리카가 잡아야 하는 요청 개수)

- Baseten의 오토스케일러는 동적으로 활성화된 레플리카를 조정하여 다양한 트래픽을 핸들링하고 아이들 연산 비용을 최소화하며 SLA를 달성하도록 한다.

    - Baseten은 특정 워크로드에 대해 적절한 파라미터 가이드를 제공하며, 프로그래밍적으로 모든 오토스케일링 옵션을 컨트롤 할 수 있도록 한다.

### Cold Start

- 콜드스타트는 GPU를 프로비저닝하고, 모델 weight를 로드하고, 이미지와 다른 의존성을 GPU에 활성화하여 트래픽을 서빙 시작하는데까지 걸리는 시간이다.

    - 이 과정에서 요청을 큐에 저장해두고, 레플리카가 활성화될때 릴리즈할 수 있어야 한다.

- 새로운 GPU들이 과거보다 훨씬 큰 VRAM을 제공하지만, 모델은 갈수록 더 커지고 있어 콜드스타트 최적화가 어느때보다도 중요해지고 있다.

    - 프로세스의 과정에서 엄청난 양의 엔지니어링이 뒤따르지 않으면, 큰 모델의 콜드스타트는 시간단위로 걸릴 수 있다.

- 빠른 콜드스타트의 장점   

    - 새로운 모델 레플리카를 빠르게 배포하며, 트래픽이 커질 때 레이턴시 스파이크를 최소화할 수 있다.

    - 초과 배포를 할 필요 없게 하며, 공격적으로 스케일 다운을 수행해 비용을 감소하도록 할 수 있다.

- 콜드 스타트 타임 최적화를 위해 다음과 타은 조합을 사용한다.

    - 병렬화 바이트레인지 다운로드를 통한 네트워크 가속

    - 로딩 타임을 가속하는 특수화 파드

    - 클러스터와 노드 레벨에서의 모델 weight, 빌드, 의존성, 데이터 캐싱

- 대부분의 모델을 초단위로, 가장 큰 모델을 분 단위로 실행할 수 있도록 한다.

### Application

- AI 어플리케이션은 멀티모달, 멀티 스테이지 복합 AI 워크로드로 만들어지며, 다양한 인퍼런스 스텝이 코디네이팅되어 작동될 수 있어야 한다.

    - 두개 이상의 모델이 있으면, 서로 다른 하드웨어와 스케일링을 필요로 한다.

    - 모놀리스로 자원을 프로비저닝하면 병목이나 초과 배포로 이어질 수 있다.

- 오토스케일링은 개별 프로세스를 독립적으로 스케일링할 수 있도록 한다.

    - 스텝마다 적절한 사이즈의 리소스로 배포하도록 한다

    - 각 AI모델을 가장 적절한 GPU에 배포하고 비지니스 로직은 CPU 인스턴스에 배포하도록 한다.

    - 파이프라인이 각 스텝을 독립적인 레플리카 개수로 배포할 수 있도록 하여 프로세싱 병목을 제거한다.

    - 요청 수가 작은 스텝에 대해 초과 배포를 피하여 비용 효율성을 확보한다.

- 트랜스크립션 워크로드를 생각하면, 오디오 청킹 -> 트랜스크립션으로 프로세스를 분리할 수 씨다.

    - 청킹은 CPU로, 두번째만 GPU로 돌리고 독립적으로 스케일링 하도록 할 수 있다.

    - 청킹이 병렬로 작동하여 아이들 상태의 GPU를 막고, 병목을 막는다.

    - 스텝마다 적절한 하드웨어를 사용하도록 한다.

### Disaggregating Prefill and Decode

- Baseten은 낮은 지연 시간의 복합 AI시스템을 위한 SDK와 툴킷을 만들었다.

    - 가장 빠르고 비용 효율적인 Whisper 트랜스크립션을 만들었다.

- Prefill과 Decode Phase

    - Prefill phase : 입력 프롬프트 전체와 대화 기록을 포함한 컨택스트를 로드하고 KV 캐시를 최초 빌드

    - Decode phase: 응답을 토큰 하나씩 생성한다. KV 캐시를 빠른 토큰 예측을 위해 사용한다.

- Prefil과 decode는 다른 자원을 필요로 하고, 다른 커널 최적화로 이점을 얻는다.

    - Prefill과 Decode의 각 단계를 분리된 GPU로 실행하여 분해하여 독립적으로 스케일링하여 레이턴시를 감소시킬 수 있다.

        - Prefill에 더 많은 GPU를 할당해 TTFT를 감소하도록 할 수 있다.

    - 인퍼런스는 런타임의 페이즈 특화 최적화와 페이즈간 자원 경쟁 제거로 전반적으로 빨라질 수 있다.

### Multi-cloud

- 멀티 클라우드 관리는 자동화, 툴, 다양한 CSP와 리전으로의 컴퓨팅 자원 프로비저닝과 운영에 대한 표준화되며 반복적으로 수행할 수 있는 지침을 필요로 한다

- Baseten 멀티 클라우드 매니지먼트는 다음을 제공한다.

    - active-active reliability를 통한 99.99%의 가용성

    - 유연한 연산 할당으로 제공할 수 있는 최선의 레이턴시 w게오

    - 데이터 residency와 sovereignty 요구사항 지원

    - 고객에게 긍정적인 비용당 성능 제공

- CSP, 리전과 고객의 요구사항과 관계 없이 일관된 퍼포먼스를 제공하는 것이 목표

- Active-active reliability

    - 여러 개의 레플리카나 리전이 live로 동시에 트래핑을 서빙하도록 하는 시스템 아키텍처

        - 한쪽의 성능이 떨어지면 다른 쪽이 방해 없이 요청을 받도록 하여 고가용성과 장애 저항성을 심리스하게 확보한다.

    - 가용성과 경제성을 다음과 같은 방법으로 개선한다.

        - CP, 리전, 노드 레벨 장애에 대해 완충 지대를 만든다

        - 워크로드를 컴퓨팅 환경 전반에 밸런싱하여 적절한 비용 프로파일과 SLA 보장을 제공한다.

        - 여러 CP에 워크로드를 분산하여 더 많은 컴퓨팅 환경에 접근할 수 있도록 한다.

    - Baseten 인퍼런스 최적화 인프라는 active-active reliability를 핸들링하여 이러한 개선을 제공하도록 한다.

        - 최적의 워크로드 타이밍과 우치를 결정한다.

- 클라우드간 오케스트레이션은 active-active reliability, GPU 오류, 멀티클라우드 스케일링을 어떻게 다룰지에 관한 이야기이다.

    - 넓은 영역에서는 CSP 사이에 일관된 추상화를 구축하여 프로비저닝과 할당이 프로바이더에 상관없이 같도록 하는 것

    - 각 CSP는 네트워크 스택부터 자원 SKU, 연관된 CPU, 램, 스토리지 등 특징들이 있다.

        - 클라우드간 오케스트레이션의 최종 목표는 높은 가용성 SLA를 만족하며 어떤 CSP에서 사용하던 GPU도 대체 가능하도록 구성하는 것.

    - 국제적 저항성과 프로바이더 레벨 및 지역적 장애로부터 보호를 보장하고, 더 많은 컴퓨터 자원으로의 접근을 제공하며, GPU 자원의 on-demand 가용량을 늘린다.

### Multi-node

- 모든 인프라의 챌린지는 모델이 최대 8개의 GPU나 싱글노드에서 작동할거라는 가정이다.

    - 몇 모델은 8개 이상의 GPU로 작동해야 하고, 멀티 노드 인퍼런스를 필요로 한다.

- 멀티노드 인퍼런싱은 다음과 같은 챌린지가 있다.

    - 상호 연결되어 있는 GPU 노드들의 라이프사이클을 관리하는 것은 관리에 복잡성을 더한다.

    - 서로 다른 CP가 서로 다른 연결 기술과 네트워킹 기술을 사용한다.

- Baseten 인퍼런스는 이러한 도전을 추상화하고 매우 큰 모델을 멀티노드에서 돌릴 수 있도록 한다.

### Security and Compliance

- 미션 크리티컬 어플리케이션에 사용되는 AI 모델은 보안과 컴플라이언스가 속도와 신뢰성보다 중요하다.

    - Baseten 인퍼런스 스택의 모든 레이어에 보안과 컴플라이언스가 내장되어 있다.

- GDPR, HIPAA, SOC 2 Type 2를 지키며, 컴플리언스 서비스 개발을 지원한다.

    - 지역 기반 라우팅은 데이터 소버린 관련 컴플리언스를 지웒나다.

    - 모든 요청은 워크로드 플레인으로 직접적으로 제공된다.

- 사용자의 데이터를 관리하지 않는다. 모델 입/출력, 하중을 저장하지 않는다.

    - 하중 캐싱은 선택적이며, 완전히 지워질 수 있다.

- 데이터와 워크로드를 지키기 위한 추가적인 강건한 보안 기능을 지원한다.

    - 데이터 암호화, 컨테이너 보안, 네트워크와 접근 제어, 워크로드 격리와 강력한 관통 테스트를 지원

    - 워크로드를 보호하기 위한 필수적인 격리를 컨테이너, 네트워크 보안, 엄격한 우선순위로 제공한다.

# The future of High-performance AI inference

- 프로덕션의 레이턴시, 모델 퍼포먼스, 가용성, 비용을 지속적으로 개선하는 것은 인퍼런스의 성능 연구와 분산 인프라에 대한 홀리스틱 뷰를 필요로 한다.

- Baseten 인퍼런스 스택은 인퍼런스 런타임을 결합하여 모델 최적화와 인퍼런싱 최적화 인프라를 결합

- 바닥에서부터 비슷한 성능과 신뢰성을 확보하는 것은 어렵다.

    - 요청 라우팅, 오토스케일링, 가용성 관리와 런타임 최적화, 커스텀 커널, speculation 엔진을 멀티 리전, CP에서 w가동시켜야 한다.

- 새로운 모델에 대해 발표 당일부터 지원하고, 각 GPU 세대마다 발전된 런타임을 제공하며, 최신 성능 연구 결과를 제공한다.

- 지속적인 개선을 통해 모델을 SOTA급으로 제공할 수 있다.

