
- 다양한 fault에 대해 다루었지만, 현실 세계의 분산 시스템은 더 어렵다.
  - 모든 것이 잘못될 수 있고, 잘못될 것이라는 최대치의 비관론이 필요하다.
- 분산 시스템 작업은 단일 컴퓨터에서 작동하는 소프트웨어와는 근본적으로 다르다.
  - 가장 큰 차이점은 잘못될 수 있는 새롭고 익사이팅한 포인트가 넘친다는 것이다.

# Faults and Partial Failures

- 단일 컴퓨터에서 프로그램을 작성하면, 일반적으로는 예상한 대로 동작한다.
  - 소프트웨어의 버그는 운이 나쁘거나, 코드를 나쁘게 작성해서 발생한다.
  - 소프트웨어가 flaky할 근본적인 이유가 없다.
    - 하드웨어가 정상적이면 결정론적으로 작동한다.
    - 하드웨어가 비정상적이면 커널 패닉같은 시스템 전반의 failure로 이어진다.
  - 완전히 작동을 안하거나 모든게 잘 적동하거나 둘 중 하나이고, 사이로 가진 않는다.
  - 컴퓨터 설계상의 의도적인 선택으로, 내부적인 오류가 발생했을 때 잘못된 결과를 반환하기보단 컴퓨터에서 crash가 발생하도록 구성한다.
    - 잘못된 결과를 반환하는 것은 어렵고, 다루기 혼란스럽기 떄문이다.
    - 컴퓨터는 fuzzy한 물리적 현실을 가리고 수학적으로 완벽한 이상화된 시스템 모델을 제공한다.
- 네트워크로 연결된 여러 대의 컴퓨터에서 동작하는 소프트웨어는 근본적으로 상황이 다르다.
  - 분산 시스템에서는 더이상 이상화된 시스템 모델에서 운영되지 않는다.
    - 물리적 세계에서, 엄청나게 다양한 것들이 잘못될 수 있다.
  - 분산 시스템에서 시스템의 일부만 예측 불가능한 방향으로 오류가 발생할 수 있다.
    - 이러한 부분적 failure는 비결정론적이다.
    - 여러 개의 노드와 네트워크가 연관되어 무언가를 수행할 때 어떨땐 작동하고 어떨땐 예측 불가능하게 실패한다.
      - 네트워크 상을 돌아다니는 메시지조차 비결정론적이기 때문에 실패했는지 아닌지 여부조차 파악할 수 없다.

## Cloud Computing and Supercomputing

- 거대한 스케일의 컴퓨팅 시스템 구성에는 철학의 스펙트럼이 존재한다.
  - 한쪽 극단은 고성능의 컴퓨팅이다.
    - CPU 수천개가 달리고 날씨 예측같은 거대한 과학적 컴퓨팅 작업을 돌린다.
  - 다른쪽 극단은 클라우드 컴퓨팅이다.
    - 멀티 테넌트 데이터센터로, IP 네트워크로 연결되며 탄력적/온디맨드 자원 할당과 측정 빌링을 지원한다.
  - 전통적인 데이터센터는 두 극단 사이 어딘가에 존재한다.
- 이러한 다른 철학에 따라 fault 처리에 다른 접근을 사용하게 된다.
  - 슈퍼컴퓨터에서 작업은 durable한 스토리지에 체크포인트로 state를 저장한다.
    - 노드 중 하나가 실패하면, 클러스터 전체 워크로드를 중단하고 노드가 복구된 후 마지막 체크포인트로부터 다시 연산을 시작한다.
    - 분산 시스템보다 단일 노드 컴퓨팅처럼 동작한다. 시스템에 부분적인 문제가 발생하면 시스템 전반의 문제로 격상시켜 처리한다.
  - 인터넷 서비스 구현을 위한 시스템은 다른 특징을 갖고, 따라서 시스템과 다른 방법을 사용해야 한다.
    - 인터넷 연관 앱은 온라인으로, 낮은 레이턴시로 유저들에게 서빙할 수 있어야 한다.
      - 복구를 위해 서비스를 중단하는 등의 서비스 비활성화가 허용되지 않는다.
    - 컴퓨팅 설계 차이가 존재한다.
      - 슈퍼컴퓨터는 각 노드가 일정 수준의 신뢰도를 갖고 노드가 공유 메모리와 원격 직접 메모리 접근 (RDMA)로 통신할 수 있는 특수화된 하드웨어로 설계된다.
      - 클라우드의 노드는 상용 머신으로 구성되어, 큰 스케일에서 낮은 비용으로 같은 성능을 낼 수 있지만 높은 failure율을 갖게 된다.
    - 거대 데이터센터 네트워크는 IP와 이더넷에 기반하며, Clos 토폴로지로 조정되어 높은 양방향 대역폭을 제공한다.
      - 슈퍼컴퓨터는 HPC 워크로드에서 더 성능이 좋은 통신 패턴인 다차원 메쉬, toruse등의 특수한 네트워크 토폴리지를 사용한다.
    - 시스템이 커질수록, 구성 요소가 깨질 가능성이 높아진다.
      - 깨진 구성요소가 고쳐지고, 새로운 구성요소가 깨지겠지만, 수천개의 노드 환경에서 어떤것이든 항상 깨진다고 생각할 수 있다.
      - 에러 핸들링 전략이 포기를 포함하면, 큰 시스템은 유용한 일을 하는것보다 fault 회복에 더 긴 시간을 소모하게 된다.
    - 지역에 따라 분산된 배포환경에서 인터넷으로 통신이 이루어진다.
      - 로컬네트워크보다 느리고 신뢰도가 낮다
      - 슈퍼컴퓨터는 모든 노드를 가깝게 위치시킨다.
- 분산 시스템이 작동하게 하려면 부분적 failure의 가능성을 인정하고, fault-tolerance 메커니즘을 소프트웨어에 탑재해야 한다.
  - 신뢰할 수 없는 구성요소로 신뢰할 수 있는 시스템을 만들어야 한다.
- 적은 수의 노드로 이루어진 작은 시스템도 부분적 실패에 대해 고려해야 한다.
  - 작은 시스템에서는 대부분의 구성 요소가 대부분의 시간동안 정상작동할거다.
  - 그럼에도 시스템 일부 구성요소가 Faulty해질거고, 소프트웨어는 이를 핸들링해야 한다.
  - fault handling은 소프트웨어 디자인의 일부여야 하고, 관리자는 소프트웨어가 fault를 어떻게 핸들링하는지 알아야 한다.
- fault는 거의 발생하지 않고 최선의 상황만을 행각하는 것은 바람직하지 않다.
  - 다양한 범위에 대해 가능한 fault에 대해 고려해 두고, 인공적으로 그러한 상황을 만들어 테스트를 해보는 것이 좋다.

## Building a Reliable System from Unreliable Components

- 신뢰도가 낮은 기반으로 신뢰성 높은 시스템을 만드는 아이디어가 존재한다.
  - 에러 교정 코드느 디지털 데이터가 통신중에 일부 비트가 손실되어도 정확하게 전달될 수 있도록 한다.
  - IP는 누락, 지연, 복제, 순서 이탈 문제로 신뢰도가 낮다.
    - TCP를 이용해 IP 레이어 위에서 통신의 신뢰도를 높인다.

# Unreliable Networks

- 이 책에서는 shared-nothing 분산 시스템, 컴퓨터가 개별적인 메모리와 디스크를 가지며 다른 컴퓨터와 메모리, 디스크를 공유하지 않는 시스템을 다룬다.
- Shared-nothing 이 시스템을 구성하는 유일한 방법은 아니지만, 인터넷 서비스 구축에서 지배적인 방식이다.
  - 특수한 하드웨어가 필요하지 않아 저렴하며, 상용 클라우드 컴퓨팅으로 구현할 수 있고, 분산된 복수의 지역의 데이터센터에 중복으로 구성하여 높은 신뢰성을 달성할 수 있다.
- 인터넷과 대부분의 데이터센터 내부 네트워크는 비동기적 패킷 네트워크이다.
  - 네트워크는 도착 시점에 대한 보장을 하지 않는다.
  - 이러한 특성으로 인해 요청을 보냈을 때 다음과 같은 문제가 발생할 수 있다.
    1. 요청이 분실될 수 있다 (실수로 랜선 뽑는 등의 문제)
    2. 요청이 큐에서 기다리다가 이후에 전달될 수 있다. (네트워크와 서버가 과부하된 경우)
    3. 원격 노드에 failed 상태일 수 있다(크러쉬, 전원 내려감)
    4. 원격 노드가 응답을 일시적으로 멈추고 이후 다시 응답을 재개하는 경우도 있다. (Stop-the-world하는 GC같은게 작동)
    5. 원격 노드가 요청을 처리했으나, 네트워크에서 응답이 소실될 수 있다. (네트워크 스위치 설정 오류)
    6. 원격 노드가 요청을 처리했으나, 응답 전달이 지연되어 이후 전달될 수 있다. (네트워크나 요청 노드가 과부하되었을 때)
- 요청측은 패킷이 전달되었는지도 확인이 불가능한다.
  - 응답측이 응답 메시지를 보내 지연이나 분실되었는지 확인해야 한다.
  - 이러한 이슈의 원인은 비동기 네트워크에서 구분이 불가능하다.
    - 다른 노드로 요청을 보내고 응답이 안왔을 때 이유를 알아내는게 불가능하다.
  - 이러한 이슈를 핸들링하기 위해서 타임아웃을 사용한다
    - 일정 시간이 지난 후 기다리는 것을 포기하고 요청이 전달되지 않았다고 여긴다.
    - 타임아웃이 발생해도 원격 노드의 요청이 받았는지 여부는 모른다.
      - 만약 어딘가 큐에 쌓여있었다면, 요청자가 포기해도 응답자로 전달된다.

## Network Faults in Practice

- 컴퓨터 네트워크는 몇십년째 만들고 있고, 신뢰성 있길 기대하겠지만 달성하지 못했다.
- 네트워크 오류가 제어된 환경에서도 놀라울 정도로 일반적이라는 시스템 연구와 입증되지 않은 근거가 있다.
  - 한 연구는 중형 데이터센터에서 한달에 12개의 네트워크 fault가 발생하며, 이 중 절반은 단일 장비 연결을, 절반은 전체 랙의 연결을 끊었다고 한다.
  - 다른 연구는 랙 최상단 스윛, 통합 스위치, 로드밸런서의 failure 율을 살펴보았고, 추가적인 네트워크 장비가 예상만큼 fault를 줄이지 못한다는 결과를 냈다.
    - 장애의 주요 원인인 휴먼 에러를 방어할 수 없기 때문
- 누구도 네트워크 문제에서 자유로울 수 없고, 기상천외한 네트워크 에러들이 많다.
  - 스위치 소프트웨어 업데이트하다가 생긴 문제가 네트워크 토폴로지 재구성으로 이어져 분 단위의 장애로 이어질 수 있다.
  - 상어가 해저케이블 깨물어서 데미지가 갈 수 있다.
  - 네트워크 인터페이스가 인바운드는 모두 드랍하면서 아웃바운드 정상적으로 보내는 경우도 있다
- 네트워크 오류가 환경에서 일반적이지 않아도 발생할 수 있다는 가능성만으로도 소프트웨어가 fault를 핸들링할 수 있어야 한다.
  - 네트워크 fault의 에러 핸들링이 정의되고 테스트되지 않으면, 문제가 생길 수 있다.
    - 클러스터 데드락걸려서 네트워크 복구되어도 영구적으로 서버 요청 못받을 수 있다.
    - 모든 데이터를 삭제할 수도 있다.
  - 소프트웨어가 예측되지 않은 상황에 놓이면, 예측되지 않은 행동으로 이어진다.
- 네트워크 fault를 다루는 것이 tolerating하는 것을 의미하지 않는다.
  - 네트워크가 일반적으로 수준의 합리적인 신뢰성이 있다면, 문제가 있을 때 네트워크 에러 메시지를 유저에게 띄우는 것으로 충분하다.
  - 소프트웨어가 네트워크 문제에 어떻게 대응하는지 알아야 하며, 네트워크 문제로부터 회복할 수 있도록 보장해야 한다.
  - 네트워크 문제를 트리거하고 시스템 응답을 살펴보는 것도 도움이 된다.
- Network Partitions
  - 네트워크의 일부가 fault로 인해 전체에서 떨어져 나가면 이를 network partition혹은 netsplit이라 한다.

## Detecting Faults

- 많은 시스템이 자동으로 문제 있는 노드를 탐지한다.
  - 로드 밸런서는 노드가 죽으면 요청을 보내면 안된다.
  - 싱글리더 분산 DB에서 리더가 죽으면 팔로워 중 하나가 새로운 리더로 선출된다.
- 네트워크의 불확실성이 노드가 작동중인지 여부를 판단하기 어렵게 한다.
  - 노드에 도달했는데, 프로세스가 도착지 포트에서 리스닝하지 않을 수 있다.
    - OS는 RST나 FIN을 보내 요청을 닫거나 거절한다.
    - 요청을 처리하던 중 노드가 죽으면 원격 노드에 얼마나 많은 데이터가 처리되었는지 알 수 없다.
  - 노드 프로세스가 죽었는데 OS는 작동하면 다른 노드로 crash를 알려 타임아웃을 기다리지 않도록 하는 스크립트를 만들 수 있다.
  - IDC 내부 네트워크 스위치의 관리 인터페이스에 접근할 수 있으면, 하드웨어 레벨에서 링크 failure를 발견하도록 쿼리할 수 있다.
    - 인터페이스로 접근할 수 없으면 불가능하다.
  - 연결하려는 IP주소가 도달 불가능하다고 확신하면 라우터는 ICMP Destination Unreachable 패킷을 보낸다.
    - 라우터도 마법의 failure 탐지 기능을 갖고 있지는 않다.
- 원격 노드의 장애를 빠르게 피드백 하는건 유용하나, 의존할 수 없다.
  - TCP가 패킷이 전달되었음을 인지해도, 어플리케이션이 핸들링 전에 죽었을 수 있다.
  - 요청이 성공임을 보장하려면, 어플리케이션 자체에 긍정적인 응답이 필요하다.
- 대조적으로, 문제가 발생하면 스택 레벨 어딘가에서 오류 응답이 발생한다.
  - 일반적으로는 문제 발생시 응답이 없다고 가정한다.
  - 몇번 Retry를 수행하고, 타임아웃까지 기다린 후, 응답이 없으면 노드가 죽었다고 판단한다.

## Timeouts and Unbounded Delays

- 타임아웃이 fault 탐지를 위한 유일한 확실한 방법일 때, 길이 설정에 대한 단순한 정답이 없다.
  - 타임아웃 시간이 길면 노드가 죽었다고 선언하는 시간이 길다는 것이다.
  - 짧은 타임아웃은 fault를 빠르게 탐지하지만, 일시적으로 느려진 것을 fault로 판단할 가능성이 높아진다.
- 미성숙한 노드 사망 선언은 문제가 된다.
  - 실제로 노드가 살아있고 액션을 수행하는 중이었는데 다른 노드가 이 작업을 받으면 동일한 작업을 여러 번 수행하게 된다.
- 노드가 사망했다고 선언되면, 책임이 다른 노드로 변경되어 다른 노드와 네트워크로 추가적인 로드를 위치시켜야 한다.
  - 시스템이 이미 높은 부하를 갖고 있다면, 미성숙한 노드 사망 선언은 문제를 악화시킬 수 있다.
  - 노드가 실제로 죽지 않고 부하로 인해 응답이 느려진 상태라면, 이 로드를 다른 노드로 전달하는 것 자체가 연쇄적인 failure로 이어진다.
- 패킷에 대한 최대 딜레이가 보장되는 네트워크를 가진 시스템을 가정하자.
  - 모든 패킷은 시간 d 안에 전달되거나 분실되며, d보다 오래 걸리지 않는다.
  - failure가 아닌 노드들은 시간 r에 모든 요청을 처리한다고 하자.
  - 이 경우, 모든 성공적인 요청이 2d + r 안에 수행되고, 넘어가면 네트워크가 혹은 원격 노드에 장애가 발생함을 알수 있다.
  - 가정이 성립할 경우 2d+r을 타임아웃으로 설정한다.
- 아쉽게도 대부분의 시스템은 위의 가정 중 어떤것도 보장하지 않는다.
  - 비동기적 네트워크는 unbounded delay를 가진다.
    - 패킷을 최대한 빠르게 전달하려 하지만, 패킷의 도착 시간에 상한이 없다.
  - 대부분의 서버 구현은 요청을 특정 최대시간안에 처리하도록 할 수 없다.
  - failure 감지에서, 시스템이 대부분 시간에 빠른것은 충분하지 않다.
    - 타임아웃 시간이 낮으면, 라운드트립 시간의 급증으로 시스템의 안정성이 깨질 수 있다.

### Network congestion and queueing

- 컴퓨터 네트워크에서 패킷 딜레이의 가변성은 큐잉에 따라 달라진다.
  - 서로 다른 노드가 동시에 같은 목적지로 패킷을 전송하면, 네트워크 스위치는 큐에 쌓아두고 목적지지로 하나씩 전송한다.
    - busy 상태 네트워크 링크에서, 패킷은 슬롯을 얻을 때 까지 기다린다.
    - 입력되는 데이터가 많아 스위치 큐가 차면, 패킷이 드럽되어 재전송되어야 한다.
  - 패킷이 목적지에 도달했을 때, 모든 CPU 코어가 busy하면 OS는 네트워크로부터 온 요청을 큐잉하고, 어플리케이션이 핸들링 할 수 있을때까지 기다린다.
    - 머신에 걸린 로드에 따라 시간이 달라진다.
  - 가상화 환경에서, 실행중인 OS는 다른 가상 머신이 CPU를 사용할 때 10ms단위로 정지된다.
    - 이 시간동안 VM은 데이터를 네트워크로부터 소비하지 못하고, 가상 머신 모니터에 큐잉된다.
      - 네트워크 딜레이의 편차를 키운다.
  - TCP는 플로우 컨트롤 (백프레셔)를 수행한다.
    - 노드가 전송하는 속도를 제한해 네트워크 링크나 리시빙 노드의 오버로딩을 피한다.
    - 데이터가 네트워크로 진입하기 이전에 전송자 레이어에서 큐를 추가한다.
- TCP는 일정 시간동안 인지되지 않으면 패킷이 소실된 것으로 본다.
  - 소실된 패킷은 자동으로 재전송된다.
  - 어플리케이션이 패킷 소실과 재전송을 보진 않지만, 결과적으로는 딜레이가 발생한다.
- TCP versus UDP
  - 레이턴시가 중요한 컨퍼런싱 혹은 VoIP같은 어플리케이션은 TCP 대신 UDP를 사용한다.
  - 신뢰도와 딜레이의 가변성 간의 트레이드오프가 된다.
    - UDP는 플로우 컨트롤을 수행하지 않고 잃어버린 패킷을 재전송하지 않으며 가변적인 네트워크 딜레이를 발생시키는 원인을을 피한다.
  - UDP는 지연된 데이터가 무용인 경우 좋은 선택이다.
    - VoIP같은 전화는 다시 전송할 필요가 없이 그냥 묵음 처리하면 된다.
- 이러한 요소들은 모두 가변적인 네트워크 딜레이로 이어진다.
- 큐잉 지연은 시스템의 최대 capacity에 가까워질 때 범위가 넓어진다.
  - 사용률이 낮은 경우 쉽게 큐를 비울 수 있지만, 사용률이 높은 경우 큐가 빠르게 길어질 수 있다.
- 퍼블릭 클라우드와 멀티테넌트 IDC에서 자원은 많은 고객들에게 공유된다.
  - 네트워크 링크와 스위치, 각 머신의 네트워크 인터페이스와 CPU가 공유된다.
  - MapReduce같은 배치 워크로드도 쉽게 네트워크 링크를 채울 수 있다.
  - 공유 자원에서 다른 고객에 대한 컨트롤이 없기 때문에, 네트워크 지연은 주변에 자원을 많이 쓰는 고객이 있으면 가변성이 굉장히 크다.

## Synchronous Versus Asynchronouse Networks

- 분산 시스템은 고정된 최대 지연시간으로 드랍 없이 네트워크에 패킷 전달을 수행할 수 있으면 훨씬 간단해진다.
  - 하드웨어 레벨에서 문제를 해결하여 네트워크의 신뢰성을 높여 소프트웨어가 관리 안하도록 하지 않는 이유?
  - 신뢰도가 굉장히 높고 지연과 소실이 없는 전통적인 고정 라인 전화 네트워크의 예시를 살펴보자.
    - 전화는 지속적인 낮은 엔드투엔드 레이턴시와 오디오 샘플 전달을 위한 충분한 대역폭을 필요로 한다. 컴퓨터 네트워크에선 사용 못하나?
    - 전화 네트워크로 전화를 하면, circuit을 성립시킨다.
      - 고정되며 대역폭의 일정량이 보정된 전체 경로가 두 콜러사이에 생성된다.
      - ISDN 네트워크는 초당 4000 프레임의 고정 레이트로 동작한다.
    - 전화가 연결되면, 각 프레임은 프레임당 15비트가 할당된다.
      - 250 마이크로초마다 16비트의 오디오 데이터 전송이 보장된다.
  - 이러한 전화 통신은 동기적이다.
    - 여러 라우터를 통과해도, 이미 16비트의 홉이 네트워크에 예약되어 있어 큐등이 사용되지 않는다.
    - 큐잉이 없기 떄문에 엔드 투 엔드 레이턴시가 고정된다. bounded delay라 한다.

### Can we not simply make network delays predictable?

- 전화선의 네트워크는 TCP와 굉장히 다르다.
  - 서킷은 고정된 양의 독점적인 대역폭을 예약해서 사용하지만, TCP는 가용 네트워크 대역폭을 원하는 대로 사용할 수 있다.
  - TCP는 가변 사이즈 블락만큼 데이터를 쓸 수 있고, 가장 빠른 방법으로 전송한다.
  - 연결이 idle이면 대역폭을 사용하지 않는다.
- 데이터센터 네트워크가 서킷 스위치 네트워크로 연결되면, 라운드트립 시간의 최댓값을 보장할 수 있을 것 처럼 보인다.
  - 실제로는 아니다. 이더넷과 IP는 패킷 스위치 프로토콜을 사용하고, 큐잉과 unbounded delay가 발생한다.
- 왜 데이터센터에선 서킷 네트워크를 사용하지 않을까?
  - 버스트성 트래픽에 최적화하기 위해서이다.
  - 서킷은 통화로 고정된 수의 비트를 일정 기간동안 전달하기에 적절하다.
    - 웹 페이지를 요청하고 이메일을 보내거나 파일을 전달하는 것은 특정한 대역폭 요구사항이 있지 않다.
    - 그냥 최대한 빠르게 전달하기만 하면 된다.
- 서킷으로 파일을 전달할 땐, 대역폭 할당을 추측해야 한다.
  - 대역폭이 너무 낮으면 서킷이 너무 느려 네트워크을 충분히 활용하지 못한다.
  - 대역폭이 너무 크면 서킷이 네트워크 상에 할당이 보장되지 않아 세팅이 안된다.
  - 서킷으로 버스트성 데이터를 전송하면 네트워크 용량을 낭비하며 전송을 느리게 한다.
    - TCP는 동적으로 데이터 전송 속도를 네트워크에 맞춘다.
- 서킷 스위칭과 패킷 스위칭을 모두 지원하는 하이브리드 네트워크같은 접근도 존재한다.
  - InfiniBand는 링크 레이어에서 엔드 투 엔드 플로우를 구현해 네트워크 큐잉의 필요를 없앴다.
  - QoS를 조심스럽게 사용하고 admission control을 수행하여 패킷 네트워크에서 서킷 스위칭을 구현할 수 있다.

### Latency and Resource Utilization

- 더 일반적으로 가변적인 딜레이를 동적 리소스 파티셔닝의 결과로 볼 수 있다.
- 10000개의 전화를 동시 처리할 수 있는 스위치에서, 각 서킷은 와이어에서 하나의 슬롯을 차지한다.
  - 와이어를 10000명의 유저에게 동시 제공할 수 있는 자원으로 생각할 수 있다.
  - 정적으로 자원이 분할된다. 혼자 자원을 쓰고 있어도 9999개의 슬롯은 사용되지 않는다.
    - 서킷은 더 사용 가능해도 대역폭의 고장된 양만큼만 사용한다.
- 대조적으로 인터넷은 동적으로 네트워크 대역폭을 공유한다.
  - 전송자는 패킷을 최대한 빠르게 받기 위해 다른 패킷들과 경쟁한다.
  - 네트워크 스위치는 어떤 패킷을 전송할지를 결정한다.
    - 이러한 접근은 큐잉의 단점을 갖고 있지만, 대역폭의 사용률을 최대화 한다.
    - 와이어는 고정된 비용으로, 사용률이 높아질 수록 바이트당 가격이 저렴해진다.
- CPU도 네트워크와 유사한 접근을 사용한다.
  - CPU코어를 여러 스레드에서 동적으로 사용하면, 한 스레드는 다른 스레드가 종료될 때 까지 일정하지 않은 시간동안 큐에서 기다려야 한다.
  - 이는 정적인 개수의 코어를 쓰레드에 할당하는 것보다 하드웨어의 사용률을 높인다.
- 레이턴시 보장은 정적 파티셔닝같은 특정 환경에서 달성할 수 있다.
  - 이는 utlization을 감소시키는, 비용이 커지는 방향으로 이어진다
  - 동적 자원 파티셔닝과 멀티 테넌시는 더 나은 사용률과 비용이 낮지만, 가변적인 지연시간으로 이어진다.
- 가변적인 지연시간은 네트워크의 법칙이 아니고, 비용 / 장점의 트레이드오프이다.

# Unreliable Clocks

- 시계와 시간은 중요하다. 어플리케이션은 다음과 같은 질문의 판단을 시계에 의존한다.
  - 요청 타임아웃인가? 서비스의 p99 레이턴시가 어떻게 되나? 지난 5분동안 QPS? 유저의 체류시간이 어떻게 되나? 아티클이 언제 퍼블리쉬 됐나? 리마인더 이메일을 언제 보낼까? 캐시 엔트리 언제 비울까? 로그파일 에러메시지가 찍힌 시점?
  - 1~4는 주기, 5\~8이 시점이 된다.
- 분산 시스템에서 통신이 즉각적이지 않기 떄문에 시간이 까다로운 문제가 된다.
  - 하나의 머신에서 다른 곳으로 갈때 네트워크에서 시간을 잡아먹는다.
    - 메시지를 받은 시간은 전송한 시간보다 항상 뒤이고, 지연시간이 가변적이기 대문에 얼마나 뒤인지 알 수 없다.
- 네트워크에 위치한 각 머신은 각각 실제 하드웨어 시계를 갖고 있고, 이러한 시계가 완전히 정확하지 않다.
  - 다른 머신보다 조금 빠르거나 느릴 수 있다.
  - NTP 등의 메커니즘이 시간의 싱크를 맞추기 위해 가장 일반적으로 사용된다.
    - GPS 리시버 등으로 더 정확한 싱크도 가능하다.

## Monotonic Versus Time-of-Day Clocks

- 현대 컴퓨터는 최소 2종류의 시계를 갖고 있다.
  - time-of-day와 monotonic clock
  - 둘 다 시간을 측정하지만, 두개를 구분하는 것이 중요하다.

### Time-of-day clocks

- 현재 날짜와 시간을 wall-clock 시간이라 불리는 특정 캘린더에 따라 반환한다.
- 리눅스의 clock_gettime(CLOCK_REALTIME), Java의 System.currentTimeMillis()는 epoch, 그레고리력 1970년 1월 1일 자정으로부터의 시간을 leap-second를 세지 않고 반환한다.
  - 다른 시스템은 다른 시간을 레퍼런스 포인트로 사용할 수 있다.
- 주로 NTP로 싱크되며, 하나의 머신의 타임스탬프가 다른 머신의 타임스탬프와 동일할 수 있다.
  - 로컬 시간이 NTP보다 빠르면, 시간을 돌리면서 과거로 리셋될 수 있다.
    - 이러한 점프는 elapsed time을 측정하는데 부적절하다.
- 거친 시간 보정을 갖고 있어, 과거에는 10ms 앞으로 가는 경우도 있었다.

### Monotonic clocks

- 타임아웃이나 서비스의 지연시간 등 시간 간격을 측정하는데 적절하다.
- 보정 등으로 뒤로 돌아가지 않고, 항상 앞으로만 가는 것이 보장된다.
- 하나의 모노클락을 확인한 후, 작업을 수행하고 다시 모노클락을 확인하여 시간 간격을 구할 수 있다.
  - 두 값의 차이가 걸린 시간이다.
- 시간의 값 자체는 의미가 없다. 부팅 시점 혹은 다른 임의적인 값으로부터의 시간이다.
- 여러개의 CPU 소켓으로 구성된 경우 CPU마다 별도의 타이머가 존재하며 싱크되지 않는다.
  - OS는 스레드가 다른 CPU에 스케줄되어 있어도 CPU에 불일치를 보상하고 어플리케이션 스레드에 일관된 시간을 제공한다.
- NTP도 로컬 시간이 NTP보다 빠르거나 느려 조정할 때 monotonic clock이 앞으로 간 시간만큼 프리퀀시를 조정할 수 있다.
  - NTP는 클락 레이트를 0.05%만큼 빠르게 / 느리게 조정할 수 있다.
  - 대부분의 시스템에서 ms 단위를 측정할 수 있어 monotonic clock 보정은 적절하다.
- 분산 시스템에서 monotonic clock을 elapsed time 측정에 사용하는 것은 적절하다.
  - 다른 노드의 시계 사이의 어떤 싱크도 추측하지 않고, 측정의 작은 부정확성에 민감하지 않기 때문이다.

## Clock Synchronization and Accuracy

- monotonic clock은 싱크가 필요하지 않으나, time-of-day clock은 NTP 서버나 외부 시간 소스에 싱크되어야 한다.
- 시간을 가져오는 알고리즘이 기대만큼 신뢰도 있고 정확하지 않다. 다음과 같은 문제가 있다.
  - 컴퓨터의 쿼츠 시계가 그렇게 정확하지가 않고, 빠르거나 느리게 움직일 수 있다.
    - Clock drift (움직여야 하는 것보다 빠르거나 느리게 움직이는 것)는 기계의 온도에 따라 달라진다.
    - drift로 인해 모든게 다 잘 작동하더라도 정확도에 한계가 생긴다.
  - NTP와 컴퓨터 시간이 크게 다르면 싱크를 취소하거나 로컬 시계를 리셋해야 한다.
    - 리셋 이전, 이후 사이에 시간을 관찰하던 어플리케이션은 과거로 가거나 점프하는 상황을 관측하게 된다.
  - 노드가 NTP에서 방화벽으로 막히면, 잘못된 설정을 알아채지 못할 수 있다.
  - NTP 싱크는 네트워크 딜레이만큼만 좋을 수 있고, 가변적인 패킷 딜레이가 있는 경우 정확도에 한계가존재한다.
    - 인터넷 전반에 싱크했을 때 딜레이의 최소가 35ms로, 딜레이의 스파이크가 있는 경우 초단위로 나왔다.
    - 설정에 따라 네트워크 딜레이가 커지면 NTP 연결 자체를 포기할 수 있다.
  - 윤초는 1분을 59초나 61초 길이로 세고, 이는 윤초를 고려하지 않은 시스템의 타이밍을 망친다.
    - 많은 대규모 시스템에서 윤초로 인해 크래시가 발생하며, 시간에 대한 부정확한 예측을 하는 것이 얼마나 쉬운지 알 수 있다.
    - 윤초를 처리하는 최선은 NTP 서버가 거짓말을 하도록 하는 것이다.
      - 윤초를 하루동안 조정하도록 하는 것이다. 실제 NTP 서버는 다양하게 작동한다.
  - 가상 머신에서 하드웨어 시계는 가상화되어 정확한 시간을 지키는 것이 어려워진다.
    - CPU 코어가 VM 간에 나눠지기 때문에, 각 VM은 다른 VM이 작동할 때 10ms단위로 기다려야 한다.
      - 어플리케이션은 이러한 일시 정지가 시계가 순간 이동한 것으로 인식된다.
  - 완전히 컨트롤하지 못하는 디바이스에서 소프트웨어를 작동시키면, 하드웨어 클락을 믿을 수 없다.
    - 어떤 유저는 의도적으로 하드웨어 시간을 부정확하게 설정할 수 있다.
    - 결과적으로 시간이 엄청난 과거 혹은 미래로 갈 수 있다.
- 큰 자원을 충분히 투자하여 좋은 시간 정확도를 얻는 것은 가능하다.
  - EU의 금융권 규제인 MiFID II는 고빈도 거래 펀드가 UTC 기준 100마이크로세컨드 단위의 시간 동기화를 필요로 한다.
    - 플래시 크래시등의 시장 이상 현상을 디버깅하고 시장 조작 탐지를 위함이다.
  - GPU 리시버, Precision Time Protocol, 주의 깊은 배포와 모니터링으로 달성할 수 있다.
  - 큰 노력과 전문성을 필요로 하며, 싱크가 잘못될 수 있는 경우도 많다.

## Relying on Synchronized Clocks

- 클락의 문제는 간단하고 사용하기 쉬워보이지만, 문제가 생기기 쉽다는 것이다.
  - 하루가 86400초가 아닐 수도 있고, time-of-day 클락은 시간이 뒤로 갈수도 있다. 노드마다 시간이 다를 수도 있다.
- 네트워크의 경우처럼 시계가 대체로 잘 작동하더라도, 문제가 생길 수 있다는 가정 하에 소프트웨어가 설계되어야 한다.
- 부정확한 시계로 인한 문제는 파악하기 어렵다.
  - CPU 결함이나 네트워크 설정 오류의 경우 작동에 문제가 생겨 바로 인지하고 수정할 수 있다.
  - 쿼츠 시계의 결함이나 NTP 클라이언트 설정 오류는 다른게 다 잘 작동하는 것처럼 보인다.
    - 실제는 clock drift가 계속 발생해 현실과 멀어진다.
  - 소프트웨어의 일부에 정확히 싱크된 시계가 필요한 경우, 결과에 데이터 손실부터 드라마틱한 크래쉬로 이어질 수 있다.
- 싱크된 클락이 필요한 소프트웨어를 사용하는 경우, 모든 컴퓨터의 클락 오프셋을 주의 깊게 모니터링해야 한다.
  - 클락 드리프트가 큰 노드는 클러스터에서 제거될 수 있어야 한다.
  - 이러한 모니터링은 큰 데미지를 입히기 전에 clock 고장을 해결할 수 있다.

### Timestamps for ordering events

- 두개의 클라이언트가 분산 데이터베이스로 작성할 때, 누가 먼저 도착할까?
- 클라이언트 A가 노드 1번에, 클라이언트 B가 동시에 노드 3번에 같은 데이터에 쓰기를 수행한다 하자.
  - 쓰기가 다른 노드로 복제될 때 쓰기가 발생한 노드에서 타임 스탬프로 태그된다.
  - 노드간 싱크가 3ms 이내로 되어있는 환경에서, 타임스탬프 기반의 오더링이 실패한다.
    - 노드 2번에서 두 이벤트를 받으면, 시간의 차이로 인해 두 이벤트간의 순서를 제대로 판단하지 못하고 타임스탬프가 더 늦은 발생한 이벤트를
- Last Write Wins 전략을 사용했을 때 이런 문제가 발생한다.
  - 멀티 리더 복제와 리더리스 아키텍처에서 모두 사용된다.
  - 서버가 아닌 클라이언트에서 타임스탬프를 찍어도 근본적인 문제가 해결되지 않는다.
  - 다음과 같은 문제가 있다.
    - 데이터베이스 쓰기가 미스테리하게 사라질 수 있다.
      - 느린 시계를 가진 노드는 노드간 시간 skew가 사라질 때 까지 빠른 시계를 가진 노드가 쓰기를 수행한 노드의 값에 덮어쓰기를 할 수 없다.
      - 어플리케이션에 에러로 반환되지 않은 상태로 드랍되는 데이터가 발생할 수 있다.
    - LWW는 빠르게 연달아 발생한 쓰기와 동시성 쓰기의 순서를 구분할 수 없게 된다.
      - 버전벡트 등의 추가적인 인과 트래킹 메커니즘이 인과 위반을 막기 위해 필요하다.
    - ms단위 보정을 하는 경우 두 노드가 동일한 타임스탬프로 쓰기를 독립적으로 수행할 수 있다.
      - 추가적인 타이브레이커 값이 충돌을 해결할 수 있으나, 인과 위반으로 이어진다.
- 가장 최근 값을 유지하고 다른 값을 버리는 방식으로 충돌을 해결할 수 있으나, 가장 최근의 값이 로컬 time-of-day 클락에 의존하는 것을 인지해야 한다.
  - NTP로 주의 깊게 싱크해도 싱크에 사용된 패킷의 레이턴시등을 파악할 수 없다.
- NTP 싱크를 이러한 순서 오류가 발생하지 않도록 정확하게 하는 것은 불가능하다.
  - NTP의 싱크 정확도는 네트워크 라운드트립 타임으로 제한된다.
  - 정확한 순서를 위해서는 측정하는 것보다 클락의 원본을 크게 정확하게 해야한다.
- 오실리에이터 쿼츠 크리스탈이 아닌 증가하는 카운터를 사용하는 논리적 시계는 순서 설정을 위한 안전한 대안이다.
  - 논리적 시계는 하루의 시간이나 elapse된 초를  측정하지 않고, 이벤트 간의 상대적 순서만을 측정한다.
  - 대조적으로 time-of-day와 monotonic clock은 physical clock이라 한다.

### Clock Readings have a confidence interval

- 머신의 time-of-day 클락을 마이크로초나 ns 단위로 보정하여 읽을 수 있을 수도 있다.
  - 이러한 보정에도 값이 이러한 precision에 정확하다는 의미는 아니다.
  - 실제로는 아닐 가능성이 높다. 부정확한 쿼츠 시계는 ms 단위로 부정확해질 수 있다.
    - NTP 서버와 매분 싱크해도 같은 문제가 발생한다.
    - 퍼블릭 NTP서버의 경우, 두자릿수 밀리초가 정확도의 최대고, 100밀리초 이상으로 스파이크 튀기도 쉽다.
- 시간 표현을 특정 시점 값으로 생각하는 것이 타당하지 않고, 신뢰 구간 내의 시간 범위에 더 가깝다.
  - 10.3초에서 10.5초 사이일 확률이 95%라고 판단하며, 더 정밀한 판단은 불가능하다.
  - 100밀리초 단위의 판단으로, 타임스탬프의 마이크로초는 실제로 의미가 없다.
- 불확실한 바운드는 시간의 소스를 기반으로 계산될 수 있다.
  - GPS 리시버나 원자 시계를 컴퓨터에 연결하면, 생산자의 예상 에러범위를 따른다.
  - 서버로부터 시간을 받으면, 불확실성은 마지막 싱크로부터 쿼츠의 드리프트, NTP 서버의 불확실성, 네트워크 라운드 트립 시간만큼 영향을 받는다.
- 대부분의 시스템은 이러한 불확실성을 명시하지 않는다.
  - 시스템에서 clock_gettime()을 호출하면 이러한 에러에 대한 예측을 명시하지 않는다.
  - Google Spanner의 TrueTime API가 특이한 예외로, 정확한 시간이 가능한 범위를 제공한다.
    - 현재 시간을 요청하면 earlist와 latest 두개의 타임스탬프 값을 제공한다.
    - 실제 시간이 가능한 인터벌을 명시적으로 제공한다.
      - 인터벌의 길이는 정확한 시간 소스로부터 싱크된 시간에 의해 결정된다.

### Synchronized clocks for global snapshots

- 스냅샷 격리는 단조 증가하는 트랜젝션 ID를 주로 사용한다.
  - 스냅샷보다 뒤에 발생한 쓰기는 스냅샨 트랜젝션에서 보이지 않는다.
- 싱글 노드 데이터베이스에서는 단순한 카운터가 TXID로 적절하다.
  - DB가 분산 시스템이면, 단조 증가 TXID는 코디네이션이 필요해 생성이 어려워진다.
  - TX는 인과성을 항상 반영해야 한다.
    - 트랜젝션 B가 트랜젝션 A에서 작성한 데이터를 읽으려면, TXID가 A보다 커야 한다. 그렇지 않으면 스냅샷의 consistency가 깨진다.
  - 분산시스템의 작고 빠른 트랜젝션 Id 생성이 방어할 수 없는 병목이 되었다.
- 싱크된 time-of-day 클락을 TXID로 쓸 때, 싱크가 충분히 잘되면 적절한 속성이다.
  - 문제는 클록의 정확도에 있다.
- Spanner는 데이터센터간 스냅샷 격리를 이를 통해 구현했다.
  - TrueTime API에 기반한 정확한 시간의 범위를 사용하고, 다음과 같은 관찰을 따른다.
    - 두개의 범위에서 A의 latest가 B의 earlist보다 빠를 때 A가 앞선다고 판단한다.
    - 범위가 겹칠때만 순서가 불확실해진다.
  - 트랜젝션의 타임스탬프가 인과성을 따르게 하도록 하기 위해, 읽기-쓰기 트랜젝션 전에 confidence interval만큼 기다렸다 수행한다.
    - 이렇게 하여 어떤 트랜젝션도 적절히 뒤의 시간에 데이터를 읽도록 한다.
      - 어떤 트랜젝션의 confidence interval도 겹치지 않도록 할 수 있다.
  - Spanner는 기다리는 시간을 최소화해야 했고, GPS 리시버와 원자 시계를 데이터센터에 도입하여 7ms 이내에 싱크될 수 있도록 구성했다.
- 분산 환경에서 트랜젝션 시멘틱을 위해 클록을 싱크하는것은 연구중이다.
  - 구글 이외에 연구한 곳은 없다.

## Process Pauses

- 파티션당 싱글리더가 있는 데이터베이스의 경우, 리더만 쓰기를 수용한다.
  - 노드는 어떻게 리더임을 판단하고, 안전하게 쓰기를 수행할 수 있을까?
- 한가지 옵션은 다른 리더로부터 lease를 얻는것으로, 타임아웃과 유사하다.
  - 한번의 하나의 노드만 lease를 일정 시간동안 얻을 수 있다.
  - 리더를 남기기 위해 노드는 만료되기 전에 lease를 갱신해야 한다.
    - 노드가 죽으면 renew가 멈추면서 다른 노드가 lease를 가져갈 수 있다.
- 다음과 같은 코드를 상상해보자.

  ``` java
  while (true) {
    request = getIncomingRequest();
    if (lease.expiryTimeMillis - System.currentTimeMillis() < 10000) {
      lease = lease.renew();
    }

    if (lease.isValid()) {
      process(request);
    }
  }
  ```

  - 뭐가 문제일까?
    - 싱크된 시계에 의존한다. lease의 만료 시간은 다른 머신에 설정되어 있는데, 이걸 로컬 시스템 클록과 비교한다.
      - 클록이 몇초보다 더 안맞으면, 코드가 이상해지기 시작한다.
    - 프로토콜을 로컬 monotonic 시계 사용하도록 해도, 코드가 시간을 확인한 시점과 요청이 프로세싱되는 시점을 아주 짧게 설정하는 문제가 있다.
      - 일반적으로 코드가 굉장히 빠르게 실행되나, 요청 중에 lease가 만료될 수도 있다.
        - 프로세스가 실행중에 일시 정지되는 케이스를 생각해보자. 15초동안 멈추면 만료되는거다.
- 스레드는 다음과 같은 상황에서 일시 정지될 수 있다.
  - JVM 등 많은 언어의 런타임은 가비지 컬렉터를 갖고, 모든 스레드를 중단시킨다.
    - 이런 stop-the-world는 몇분동안 지속될 수도 있다.
    - Hotspot JVM의 CMS등 "동시성" 가비지 컬렉터도 어플리케이션과 병렬로 동작할 수 없다.
      - 잠시동안 stop-the-world가 일어난다.
    - GC 세팅이나 메모리 할당 전략으로 줄일 수는 있지만, 최악을 가정하여 강건함을 보장해야 한다.
  - 가상 환경에서 VM이 서스펜드 되었다 복구될 수 있다.
    - 모든 프로세스의 실행을 일시 정지하고 메모리의 내용을 디스크에 저장하는 과정
    - 이러한 일시 정지는 프로세스 실행 과정 어디에서도 일어날 수 있고 임의의 시간동안 지속된다.
    - 이러한 기능은 리부팅 없이 호스트에서 다른 호스트로 VM의 라이브 마이그레이션을 하기 위해 사용된다.
      - pause는 어떤 프로세스가 메모리에 쓰기를 수행하는지에 따라 달라진다.
  - 노트북같은 엔드 유저 디바이스는, 노트북을 닫는 등의 경우 임의로 멈출 수 있다.
  - OS가 다른 스레드로 컨택스트 스위칭을 하거나, 하이퍼바이저가 다른 VM으로 스위칭 하는 경우, 현재 작동ㅅ하는 스레드는 코드 상의 임의의 포인트에서 일시 정지된다.
    - VM의 경우 다른 VM이 사용하는 CPU 타임을 steal time이라 한다.
    - 컴퓨터의 큰 로드가 걸린 경우, 일시정지된 스레드가 다시 작동하는데 시간이 걸린다.
  - 어플리케이션이 동기적으로 디스크에 접근하면, 디스크 IO동안 일시정지 될 수 있다.
    - 많은 언어에서 명시적으로 파일 접근을 하지 않더라도 디스크 접근이 발생할 수 있다.
      - 자바 클래스로더는 레이지로딩으로 필요한 순간에 클래스 파일을 로드한다.
    - IO pause와 GC pause는 결합될수도 있다.
    - 디스크가 네트워크 상에 블락이나 파일시스템으로 있으면 IO 레이턴시는 네트워크 레이턴시까지 늘어난다.
  - OS가 스왑을 사용하면, 단순 메모리 접근도 page fault로 이어질 수 있다.
    - page fault : 페이지가 디스크에서 메모리로 로드되어야 하는 상황
    - 쓰레드는 느린 IO가 동작할때까지 기다려야 한다.
      - 메모리 사용률이 높으면 다른 페이지가 나와야 한다.
    - 극단적인 상황에선 OS가 페이지 스와핑에 대부분의 시간을 다 쓰고 작동안한다.
    - 이를 피하기 위해 서버에서는 페이징이나 스왑을 비활성화한다.
  - 유닉스 프로세스는 SIGSTOP 시그널을 보내 일시정지될 수 있다.
    - SIGCONT로 resume될 때 까지 CPU사이클 할당을 받는 것을 멈춘다.
    - 환경에서 SIGSTOP을 일반적으로 사용하지 않아도, 엔지니어 실수로 보내질 수 있다.
  - 이 모든것이 스레드가 알지도 못하게 일시정지하고 이후 다시 시작하도록 할 수 있다.
- 이 문제는 단일 머신의 멀티 스레드 코드를 thread-safe하게 구성하는것과 유사하다.
  - 임의의 컨택스트 스위칭과 병렬이 발생해 타이밍에 대해 어떤것도 예상할 수 없다.
  - 단일 머신에서 멀티 스레드 코드를 쓰면, thread-safe하게 만들 도구가 많다.
    - mutex나 semaphore, 원자 카운터, lock-free 자료구조, 블로킹 큐 같은게 있다.
    - 분산 네트워크는 공유 메모리가 없어 이런것들을 모두 사용할 수 없다.
      - 신뢰할 수 없는 네트워크에 메시지로 모든걸 맡겨야 한다.
  - 분산 시스템의 노드는 함수 중에도 갑자기 긴 시간동안 멈출 수 있음을 가정해야 한다.
    - pause되어도 다른 세상은 작동하며, pause된 노드가 응답이 없어 죽었다고 판단할 수도 있다.
      - 결과적으로 pause된 노드가 다시 시작되며, 잠자기 상태였던 것을 파악도 못할 수 있다.

### Response Time guarantees

- 많은 언어와 OS에서 스레드와 프로세스는 확정되지 않은 시간만큼 멈출 수 있다.
  - 일시정지의 이유는 없앨 수 있다.
- 몇몇 소프트웨어는 응답이 특정시간동안 멈추면 중대한 데미지로 이어질 수 있다.
  - 비행기, 로켓, 로봇, 차같은건 센서 입력에 대해 무조건 최대한 빠르게 작동해야 한다.
  - 이러한 시스템에선 소프트웨어의 응답에 걸리는 시간에 특정한 데드라인을 설정한다..
    - 데드라인을 만족하지 못하면 모든 시스템이 failure로 이어질 수 있다.
    - 이러한 시스템을 hard real-time 시스템이라고 한다.
- Is real-time really real?
  - 임베디드 시스템에서 리얼타임은 시스템이 어떤 상황에서도 특정한 시간 내 작동을 만족하도록 보장하는 것이다.
    - 웹에서 엄격한 응답시간 제약 없이 스트리밍하거나 서버가 클라이언트로 데이터를 푸쉬하는 데에 사용하는 표현인 real-time과는 다르다.
- 시스템에서 리얼타임을 보장하려면 소프트웨어 스택의 모든 레벨에서 지원이 필요하다.
  - Real-time OS로 프로세스를 특정 길이만큼 CPU 타임 할당을 보장할 수 있어야 한다.
  - 라이브러리 함수는 최악의 경우의 실행 시간을 명시해야 한다.
  - 동적 메모리 할당은 제한되거나 완전히 비활성화 되어야 한다.
    - 리얼타임 GC가 존재하나, 어플리케이션은 GC가 많은 일을 하지 않도록 해야한다.
  - 엄청난 양의 테스트와 측정이 이루어져야 한다.
- 사용할 수 있는  프로그래밍 언어, 라이브러리, 툴의 제약부터 추가적인 작업과 제한을 필요로 한다.
  - 이러한 이유로 리얼타임 시스템 개발은 굉장히 비싸며, 안전이 중요한 임베디드 기계에만 사용된다.
  - 리얼타임 시스템은 응답 시간을 다른 요소보다 우선하기 때문에 더 낮은 처리량을 갖는다.
- 대부분의 서버사이드 데이터 처리 시스템에서 실시간은 경제적이지 않거나 부적절하다.
  - 결과적으로 리얼타임이 아닌 시스템의 pause와 시간의 불확실성에 계속 시달려야 한다.

### Limiting the impact of garbage collection

- 프로세스 일시 정지의 부정적 영향은 리얼타임 스케줄링 보장 없이도 완화할 수 있다.
  - 런타임은 객체 할당률이나 남은 빈 메모리 양을 알 수 있어 GC 스케줄링에 유연성을 갖고 있다.
- GC를 노드의 계획된 장애로 다루고, 다른 노드가 요청을 핸들링하도록 하는 것이다.
  - 런타임이 어플리케이션에 GC pause에 도입한다고 하면, 어플리케이션은 그 노드로 요청을 보내지 않도록 할 수 있다.
  - 어플리케이션은 새로운 요청을 수용하지 않고 기다리는 요청을 처리한 후 아무런 요청 없이 GC를 수행한다.
    - 클라이언트로부터 GC를 숨겨 높은 퍼센타일의 응답 시간을 낮춘다.
    - 금융 트레이딩 시스템 등 레이턴시에 민감한 시스템이 이를 사용한다.
- 위 아이디어의 변종으로 가비지 컬렉터를 짧은 수명의 객체에 대해서만 작동시키고 주기적으로 프로세스를 재식하는 방법이 있다.
  - 큰 객체까지 full GC 할만큼 쌓이기 전에 프로세스를 재시작하는 것이다.
  - 한 노드는 재시작할 수 있고, 예정된 재시작동안 다른 노드에서 트래픽을 받을 수 있다.
- GC pause를 완전히 방지할 수는 없으나, 어플리케이션에서의 임팩트를 줄일 수 있다.

# Knowledge, Truth, and Lies

- 분산 시스템은 단일 컴퓨터와 다르고, 이러한 이슈의 결과는 분산 시스템과 익숙하지 않으면 혼란이 발생한다.
  - 네트워크 안에 있는 노드는 모든 것을 확실하게 알지 못한다.
  -
